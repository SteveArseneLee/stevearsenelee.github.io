[{"id":0,"href":"/docs/infra/kvm/whatswrong/","title":"이걸 해볼까요","section":"KVM","content":" Worker Node에 Role 부여하기 # kubectl label node {node명} node-role.kubernetes.io/worker=worker NFS 서버 연결 및 PersistentVolume, StorageClass 설정 # NFS 서버 확인 IP 확인 공유 디렉토리 : e.g. /srv/nfs/k8s NFS 서버 설치하기\nsudo apt update \u0026amp;\u0026amp; sudo apt install nfs-kernel-server -y sudo mkdir -p /srv/nfs/k8s sudo chown nobody:nogroup /srv/nfs/k8s echo \u0026#34;/srv/nfs/k8s *(rw,sync,no_subtree_check,no_root_squash)\u0026#34; | sudo tee -a /etc/exports sudo exportfs -rav sudo systemctl restart nfs-kernel-server NFS Client 설치\n모든 워커 노드에 아래 실행 sudo apt update \u0026amp;\u0026amp; sudo apt install nfs-common -y kubespray # [kube_control_plane] k8s-m1 ansible_host=192.168.0.101 ip=192.168.0.101 etcd_member_name=etcd1 k8s-m2 ansible_host=192.168.0.102 ip=192.168.0.102 etcd_member_name=etcd2 k8s-m3 ansible_host=192.168.0.103 ip=192.168.0.103 etcd_member_name=etcd3 [etcd] k8s-m1 k8s-m2 k8s-m3 [kube_node] k8s-w1 ansible_host=192.168.0.104 ip=192.168.0.104 k8s-w2 ansible_host=192.168.0.105 ip=192.168.0.105 k8s-w3 ansible_host=192.168.0.106 ip=192.168.0.106 [k8s_cluster:children] kube_control_plane kube_node "},{"id":1,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C/1-process-thread/","title":"1. Process \u0026 Thread","section":"운영체제","content":" 1. 정의 # 프로세스 (Process) # 실행 중인 프로그램의 인스턴스 운영체제로부터 독립된 메모리 공간, 자원을 할당받음 자체 주소 공간, 코드 영역, 데이터, 힙, 스택을 가짐 각 프로세스는 **PCB(Process Control Block)**로 커널이 관리 스레드 (Thread) # 프로세스 내부에서 실행되는 작업의 흐름 단위 코드, 데이터, 힙, 파일 디스크립터 등을 프로세스 내 다른 스레드와 공유 스택과 레지스터는 독립적으로 가짐 **TCB(Thread Control Block)**로 커널/라이브러리가 관리 2. 구조 및 차이점 # 항목 프로세스 스레드 정의 실행 중인 프로그램의 인스턴스 프로세스 내 작업 흐름 단위 메모리 구조 코드, 데이터, 힙, 스택 모두 독립 스택만 독립, 나머지 공유 자원 공유 없음 (IPC 필요) 있음 (공유 메모리 기반) 생성 비용 큼 (fork) 작음 (pthread_create) 컨텍스트 스위칭 주소 공간 전환 포함 → 비용 큼 주소 공간 공유 → 비용 적음 안정성 다른 프로세스에 영향 없음 하나의 오류가 전체에 영향 가능 3. Context Switching # CPU가 실행 중인 작업을 중단하고 다른 실행 단위로 전환하는 과정 필요한 정보: PC, 레지스터, 스택 포인터 등 스레드 간 전환은 주소 공간이 같아 빠르지만, 동기화 문제 발생 가능 프로세스 간 전환은 보호는 뛰어나지만, 오버헤드 큼 4. 실무 활용 예시 # 분야 설명 웹 서버 Nginx, Tomcat 등은 클라이언트 요청을 스레드 단위로 분리 처리 (스레드 풀 기반) 브라우저 Chrome: 탭은 프로세스, 탭 내 동작은 스레드로 분리 (안정성과 병렬성 모두 확보) 멀티코어 CPU 활용 Python은 GIL로 멀티 스레딩 성능 한계 → 멀티프로세싱 사용 (예: multiprocessing 모듈) 게임 엔진 렌더링/네트워크/물리 계산을 개별 스레드로 병렬 처리 5. 관련 명령어 및 실습 # 명령어 설명 ps -ef, top 현재 실행 중인 프로세스 확인 htop F2 → Threads 표시 옵션으로 스레드 확인 strace -p 해당 프로세스의 시스템 콜 흐름 추적 pthread_create() POSIX 스레드 생성 (C/C++) java.lang.Thread, Runnable Java에서의 스레드 작성 방식 6. 자주 묻는 면접 질문 # Q1. 프로세스와 스레드의 차이를 설명해주세요.\nA. 프로세스는 실행 중인 프로그램의 독립적인 단위로, 운영체제로부터 별도의 메모리 공간을 할당받는다. 반면, 스레드는 프로세스 내부에서 실행되는 흐름 단위로, 같은 프로세스의 다른 스레드들과 메모리 공간을 공유한다.\n항목 프로세스 스레드 메모리 공간 독립 (코드, 데이터, 힙, 스택) 스택만 독립, 나머지 공유 생성 비용 상대적으로 높음 가벼움 (빠른 생성, 스위칭) 통신 방식 IPC 필요 공유 메모리로 빠름 -\u0026gt; 스레드는 비용이 적고 통신이 빠르지만, 하나가 문제를 일으키면 전체 프로세스에 영향을 줄 수 있어 동기화와 예외 처리가 중요합니다.\nQ2. 하나의 스레드가 죽으면 전체 프로세스에 어떤 영향을 주나요?\nA. 스레드는 동일한 프로세스 공간 내에서 자원을 공유하기 때문에, 하나의 스레드에서 예외가 발생하거나 메모리를 오염시키면, 전체 프로세스가 비정상적으로 종료될 수 있다. 예를 들어, Java에서 한 스레드가 NullPointerException으로 죽을 경우, 예외 처리를 하지 않으면 전체 애플리케이션이 종료된다. 이를 방지하기 위해 try-catch, uncaughtExceptionHandler, 또는 서킷 브레이커 같은 전략을 사용한다.\nQ3. 멀티 프로세싱과 멀티 스레딩 중 언제 어떤 것을 선택할 건가요?\nA.\n멀티 스레딩: I/O 중심 작업, 웹 서버 등 빠른 응답이 중요하고, 자원 공유가 필요한 경우 멀티 프로세싱: 계산 집약적(CPU-bound) 작업, 병렬 처리에 적합. 예: 이미지 처리, 머신러닝 학습 특히 Python은 GIL(Global Interpreter Lock) 때문에 CPU-bound 작업은 멀티 프로세싱이 더 적합히다. 반대로 Java나 Go 같은 언어는 멀티 스레딩이 강력하게 지원된다.\n"},{"id":2,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C/2-cpu-scheduling/","title":"2. CPU Scheduling","section":"운영체제","content":" 1. 정의 # CPU 스케줄링은 Ready Queue에 있는 프로세스들 중 어떤 프로세스에게 CPU를 할당할지 결정하는 정책. 멀티태스킹 OS에서 프로세스 간 CPU를 공유하기 위한 핵심 메커니즘. OS의 핵심 기능 중 하나로, 스케줄러는 다양한 정책에 따라 프로세스를 선택함. 주요 목적 # 시스템 자원의 공정한 분배 CPU 사용률(Throughput) 극대화 대기 시간, 응답 시간 최소화 전반적인 시스템 반응성 향상 2. 스케쥴링이 필요한 시점 # 프로세스가 CPU burst를 마치고 I/O 요청 → CPU 반환 프로세스가 종료됨 선점형 스케줄러의 경우, 더 높은 우선순위의 프로세스가 도착 sleep(), wait() 호출 후 wake up될 때 3. 주요 스케쥴링 기준 # 기준 설명 CPU burst time 얼마나 짧게 CPU를 점유할지 우선순위 (Priority) 사용자/시스템 지정 도착 시간 (Arrival Time) 먼저 온 순서 대기 시간 / 응답 시간 유저가 체감하는 반응 속도 Aging 기아(Starvation) 방지 목적 4. 스케쥴링 알고리즘 # FCFS (First-Come, First-Served) 먼저 온 순서대로 처리 (Queue 기반) 단순하지만 Convoy 현상 발생 가능 (긴 작업이 짧은 작업을 막음) SJF (Shortest Job First) CPU burst가 가장 짧은 작업 우선 이론상 가장 효율적 (대기시간 최소), 하지만 예측이 어렵다 비선점형(기본) 또는 **선점형(SRTF)**으로 구현 가능 Round Robin (RR) 각 프로세스에 Time Quantum 부여 시간 할당이 끝나면 선점 발생 → 공정성 ↑ 응답성이 중요할 때 유용 (인터랙티브 시스템) Priority Scheduling 우선순위 높은 작업부터 실행 Starvation 발생 가능 → Aging 기법으로 완화 Multi-Level Queue (MLQ) 프로세스를 성격에 따라 여러 큐로 분류 (ex. interactive, batch) 큐마다 별도 알고리즘 사용 큐 간 우선순위 존재 (low queue는 starvation 가능) Multi-Level Feedback Queue (MLFQ) MLQ 확장형, 프로세스가 다른 큐로 이동 가능 초기엔 높은 우선순위에서 시작, CPU 오래 쓰면 점점 낮은 우선순위로 이동 현대 OS가 채택하는 현실적인 정책 5. 선점형 vs 비선점형 # 구분 비선점형 선점형 설명 CPU를 할당받으면 자발적으로 반환할 때까지 유지 우선순위 등 조건에 따라 중간에 선점 가능 예시 알고리즘 FCFS, SJF RR, Priority(선점형), SRTF 특징 단순, context switch 적음 응답성 우수, 복잡도 ↑ 6. 성능 지표 # 지표 설명 CPU 사용률 CPU가 놀지 않고 일하는 비율 Throughput 단위 시간당 완료된 프로세스 수 Turnaround Time 프로세스 시작 ~ 종료까지 걸린 시간 Waiting Time Ready Queue에서 기다린 시간 Response Time 요청 후 첫 반응까지 걸린 시간 7. 실무 예시 # OS 스케줄러 Linux (CFS) Completely Fair Scheduler – 시간 단위를 Weight로 변환해 공정하게 분배 Windows Multilevel Feedback Queue 기반 선점형 RTOS Priority-based preemptive scheduling (실시간성 강조) =\u0026gt; 📌 Linux CFS는 가상 런타임(VRuntime) 기반으로, 각 태스크의 실행 시간을 추적하며 가장 “덜 사용한” 프로세스에게 CPU를 할당.\n8. 자주 묻는 면접 질문 # Q1. SJF가 이상적으로 효율적인 이유는?\nA.\nSJF는 평균 대기 시간을 수학적으로 최소화하는 최적 알고리즘. 하지만 실제 CPU burst time을 정확히 예측하는 건 어렵기 때문에 실무 적용이 제한적. Q2. Round Robin의 타임퀀텀 크기는 어떻게 결정해야 하나요?\nA.\n너무 작으면 context switching 오버헤드 증가 너무 크면 FCFS처럼 변함 일반적으로 10~100ms 수준 Q3. 선점형 스케줄링이 필요한 이유는?\nA.\n실시간 반응이 필요한 환경 (GUI, 인터랙티브 shell 등) 짧은 작업이 긴 작업에 의해 밀리는 것을 방지 Q4. 멀티코어 환경에서 스케줄링은 어떻게 동작하나요?\n각 CPU에 Run Queue를 두는 per-CPU 모델 사용 부하 분산을 위한 load balancing 또는 task migration 전략 필요 Linux는 SMP(Symmetric Multi Processing) 스케줄러로 처리 "},{"id":3,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C/3-process-synchronization/","title":"3. 프로세스 동기화(Process Synchronization)","section":"운영체제","content":" 1. 정의 # 프로세스 동기화란 둘 이상의 프로세스(또는 스레드)가 공유 자원에 접근할 때 충돌 없이 안전하게 작업할 수 있도록 보장하는 방법. 주로 임계 구역(Critical Section) 문제를 해결하기 위한 동기화 메커니즘을 의미. 🔹 임계 구역 (Critical Section) # 동시에 하나의 프로세스만 접근해야 하는 공유 자원 처리 구간. 예: 전역 변수, 공유된 파일, 네트워크 소켓 등 🔹 동기화의 목적 # Race Condition 방지: 둘 이상의 프로세스가 데이터를 동시에 읽고 쓰면 비정상적인 결과 발생 일관성 유지: 데이터 무결성 보장 2. 동기화의 3가지 요구 조건(임계 구역 문제 해결 조건) # 상호 배제 (Mutual Exclusion): 하나의 프로세스만 임계 구역 실행 가능 진행 (Progress): 임계 구역에 진입하지 않은 프로세스는 진입 여부 결정에 관여 X 한정 대기 (Bounded Waiting): 무한정 대기 없이 순차적으로 기회 부여 3. 동기화 주요 기법 # 뮤텍스(Mutex, Mutual Exclusion Lock) 한 번에 하나의 스레드만 락을 획득 가능 획득한 스레드만 임계 구역 진입 → 완료 후 unlock 소유 개념 있음: 락을 걸고 해제할 수 있는 주체는 동일해야 함 사용 예시: pthread_mutex, Java synchronized, C++ std::mutex pthread_mutex_t lock; pthread_mutex_lock(\u0026amp;lock); // 임계 구역 pthread_mutex_unlock(\u0026amp;lock); 세마포어(Semaphore) 카운팅 가능한 동기화 도구 두 가지 연산: P() 또는 wait() → 자원 요청 (count–) V() 또는 signal() → 자원 반납 (count++) 음수가 되면 대기 큐에 블록됨 소유 개념 없음 → 다른 스레드가 해제 가능 종류 설명 Binary Semaphore 0 or 1 (뮤텍스와 유사) Counting Semaphore 특정 수 이상의 동시 접근 허용 가능 스핀락(Spinlock) 락을 획득할 때까지 CPU를 점유한 채 무한 루프(바쁜 대기). 컨텍스트 스위치가 비싼 커널 공간이나 짧은 락 소유 시 유리 주의: 멀티코어 환경에서만 유효하며, 싱글코어에서 사용하면 CPU 낭비 모니터(Monitor) 언어 수준의 동기화 추상화 내부에 Lock + Condition Variable 포함 Java, C#, Go 등에서 사용 (synchronized, wait/notify, etc.) 조건 변수 (Condition Variable) 어떤 조건이 만족될 때까지 기다리는 데 사용 주로 뮤텍스와 함께 사용 예시: pthread_cond_wait() Java Object.wait() / Object.notify() 4. Race COndition 예제와 해결 방법 # int counter = 0; void* increment(void* arg) { for (int i = 0; i \u0026lt; 1000000; i++) { counter++; } } 위 코드에서 두 개의 스레드가 동시에 counter++를 수행하면 Race Condition 발생 해결책: mutex 사용 5. 실무 예시 # 분야 동기화 방식 다중 요청 처리 서버 (ex. Tomcat) Thread Pool + 뮤텍스 / 세마포어 생산자-소비자 패턴 Circular Buffer + 조건 변수 DB connection pool Counting Semaphore 커널 영역 (락 없는 프로그래밍 포함) Spinlock, 원자 연산, CAS(Compare-And-Swap) 6. 자주 묻는 면접 질문 # Q1. 세마포어와 뮤텍스의 차이는? A.\n뮤텍스는 1개의 자원을 보호하며 소유 개념이 있다. 세마포어는 개수(count)를 가질 수 있고, 소유 개념이 없다. 또한 세마포어는 하나의 스레드가 wait, 다른 스레드가 signal 가능. Q2. 스핀락은 언제 사용하나요? A.\n락 소유 시간이 매우 짧고, 컨텍스트 스위칭 비용이 큰 커널 공간 또는 멀티코어 환경에서 사용. 싱글코어에서는 오히려 성능 저하. Q3. 뮤텍스를 사용하는데도 데드락이 발생하는 이유는? A.\n락의 획득 순서가 꼬이면 데드락 발생 가능. 다중 자원 요청 시 락 획득 순서 통일이 중요. 해결책: 타임아웃, 정렬된 순서로 요청, 락 순서 정책 등 "},{"id":4,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C/4-deadlock/","title":"4. Deadlock","section":"운영체제","content":" 1. 정의 # 데드락(교착 상태, Deadlock) 이란 여러 프로세스(또는 스레드)가 서로 자원이 풀리기를 기다리며 무한히 블로킹되어, 더 이상 진행되지 못하는 상태를 의미한다.\nA는 B의 자원을 기다리고, B는 A의 자원을 기다리는 순환적 대기 상태.\n2. 데드락 발생 조건(Coffman\u0026rsquo;s Conditions) # 데드락은 다음 4가지 조건이 동시에 성립할 때 발생\n조건 설명 상호 배제 (Mutual Exclusion) 자원은 한 번에 하나의 프로세스만 사용 가능 점유 대기 (Hold and Wait) 자원을 보유한 상태에서 다른 자원을 기다림 비선점 (No Preemption) 이미 할당된 자원을 강제로 뺏을 수 없음 순환 대기 (Circular Wait) 자원 대기 관계가 순환 구조를 이룸 (P1→P2→…→Pn→P1) =\u0026gt; 데드락을 방지하려면 이 중 하나라도 깨야 함 3. 데드락 예시 # Thread A: lock(a) → lock(b) Thread B: lock(b) → lock(a) -\u0026gt; A는 a를, B는 b를 먼저 얻고 서로 상대방의 자원을 기다림 → 교착 상태 발생\n4. 해결 전략(4가지 접근법) # 전략 설명 핵심 아이디어 예방 (Prevention) Coffman 조건 중 하나 이상을 사전에 제거 ex: 자원 요청 전에 모두 확보 회피 (Avoidance) 데드락 발생 가능성을 사전에 예측하고 우회 ex: 은행원 알고리즘 탐지 및 복구 (Detection \u0026amp; Recovery) 데드락 발생을 허용하고 사후 복구 ex: 자원 할당 그래프 순환 탐지 무시 (Ignore) 그냥 냅둠 (“Ostrich Algorithm”) ex: Linux 대부분 경우 무시 예방 전략 구체화 # 제거 조건 방법 상호 배제 자원을 공유 가능하게 설계 (불가능한 경우 많음) 점유 대기 모든 자원을 한 번에 요청하게 강제 비선점 자원 선점 가능하도록 설계 (중단 후 롤백) 순환 대기 자원에 고정된 순서를 부여하고 순서대로만 요청 허용 회피 전략 – 은행원 알고리즘 (Banker’s Algorithm) # 프로세스가 자원 요청 시 최악의 시나리오를 가정하고도 시스템이 안정 상태(safe state)를 유지할 수 있으면 자원을 할당 자원 할당 상태를 벡터/행렬로 표현 실무 적용은 어려움 (자원 요구량 예측이 필요) 탐지 및 복구 # 자원 할당 그래프(Resource Allocation Graph)에서 순환(Cycle) 존재 여부로 데드락 감지 복구 방법: 프로세스 강제 종료 자원 선점 → 이전 상태로 롤백 (중단 가능해야 함) FIFO 순으로 kill, 최소 비용 기준 kill 등 5. 실무 예시 # 분야 대응 방식 DB 트랜잭션 (예: MySQL, PostgreSQL) Lock wait timeout / Deadlock detection (순환 감지) Java Thread synchronized 블록 교착 가능 → 타임아웃/락 순서 강제 Go / C++ 뮤텍스 체인 락 시 교착 위험 → 락 정렬 정책 필요 커널/드라이버 Deadlock 방지 로직 삽입, 스핀락 순서 고정 6. 자주 묻는 면접 질문 # Q1. 데드락이 발생하려면 어떤 조건이 필요한가요?\nA.\n상호 배제 점유 대기 비선점 순환 대기 → 네 가지 조건이 모두 만족될 때만 데드락 발생\nQ2. 데드락을 방지하기 위해 어떤 전략이 있나요?\nA.\n예방: 네 가지 조건 중 하나 이상을 깨뜨림 회피: 안전 상태 유지 보장 (ex. Banker’s Algorithm) 탐지: 순환 관계 발견 후 해결 무시: 성능 우선 시스템에선 감수하고 방치 Q3. 실무에서 데드락을 어떻게 예방하나요?\nA.\n자원 획득 순서 고정 락 획득 시 타임아웃 설정 락 수 제한 트랜잭션 구조 최소화 "},{"id":5,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C/5-memory-manage/","title":"5. 메모리 관리","section":"운영체제","content":" 1. 메모리의 계층 구조 및 주소 체계 # 메모리 계층 구조 # CPU -\u0026gt; register -\u0026gt; cache -\u0026gt; main memory(RAM) -\u0026gt; SSD/HDD OS 입장에서는 **main memory(RAM)**을 관리하는 게 핵심 주소 체계 # 구분 설명 논리 주소(Logical Address) CPU가 생성한 주소 (프로세스 입장에서의 주소) 물리 주소(Physical Address) 실제 메모리 하드웨어 상의 주소 가상 주소(Virtual Address) 논리 주소와 같으며, MMU가 물리 주소로 변환 =\u0026gt; MMU (Memory Management Unit): 논리 주소를 물리 주소로 변환하는 하드웨어. 페이징 기반 시스템에서 핵심 역할.\n2. 메모리 단편화(Fragmentation) # 내부 단편화 (Internal Fragmentation) # 프로세스가 할당받은 메모리보다 적은 양만 사용하는 경우 ex) 12KB 필요한데 16KB 단위로만 할당 → 4KB 낭비 외부 단편화 (External Fragmentation) # 총 메모리는 충분하지만 연속된 공간이 부족해서 할당 불가 ex) 5KB, 3KB, 4KB, … 식으로 쪼개져 있어 10KB 요청 실패 해결 전략 # 문제 해결 방식 내부 단편화 가변 크기 할당 (slab, buddy, paging 등) 외부 단편화 페이징, 세그멘테이션, 압축(compaction) 등 3. 메모리 할당 기법 # 연속 할당 (Contiguous Allocation) # 프로세스는 하나의 연속된 메모리 블록을 할당받음 단순하지만 외부 단편화 발생 가능 방식 설명 단순 분할 (Fixed) 동일 크기의 파티션 가변 분할 (Dynamic) 프로세스 크기에 따라 동적 할당 할당 전략 (First Fit, Best Fit, Worst Fit)\nFirst Fit: 첫 번째로 맞는 공간 Best Fit: 가장 크기가 근접한 공간 (외부 단편화 ↑) Worst Fit: 가장 큰 공간 (파편 방지 목적) 페이징 (Paging) # 물리 메모리를 고정된 크기(Frame)로 나누고, 프로세스도 동일한 크기(Page)로 나눔 페이지 단위로 분산 저장 가능 -\u0026gt; 외부 단편화 제거 요소 설명 Page 논리 주소 공간의 단위 Frame 물리 주소 공간의 단위 Page Table Page 번호 → Frame 번호 매핑 정보 저장 MMU 주소 변환 시 Page Table 참조 =\u0026gt; 단점: Page Table 크기 증가, TLB 미스 발생 가능\n세그멘테이션 (Segmentation) # 프로세스를 의미 있는 논리 단위(코드, 데이터, 스택 등)로 나눔 각 세그먼트는 크기/시작 주소가 다름 주소 = (세그먼트 번호, 오프셋) 형태 장점 : 논리적 구조 반영\n단점 : 외부 단편화 발생 가능\n4. 논리 주소 -\u0026gt; 물리 주소 변환 과정 # 방식 변환 방식 특징 연속 할당 Base + offset 구조 단순 페이징 Page Number + Offset → Frame 고정 크기 블록, 내부 단편화 가능 세그멘테이션 Segment + Offset → Physical 논리 구조 반영, 외부 단편화 발생 5. 실무 예시 # 사례 설명 Linux 페이징 기반, 가상 메모리 + TLB 사용 Embedded 시스템 연속 할당 또는 세그멘테이션 구조 Heap Allocator 내부 단편화 방지 위해 Slab, Buddy Allocator 사용 Docker/VM 가상 주소 공간을 격리하기 위해 페이지 테이블 관리 필수 6. 자주 묻는 면접 질문 # Q1. 페이징과 세그멘테이션의 차이는?\nA.\n항목 페이징 세그멘테이션 단위 고정 크기 Page 가변 크기 Segment 단편화 내부 단편화 외부 단편화 구조 단순, 추상도 낮음 논리 구조 표현 가능 주소 형식 Page No + Offset Segment No + Offset Q2. 외부 단편화와 내부 단편화는 무엇이고, 각각 어떻게 해결하나요?\nA.\n내부 단편화: 고정 크기 할당 시, 일부 미사용 공간 → 페이징, slab allocator 외부 단편화: 가변 할당 시, 작은 조각들 → 페이징, 압축, 세그멘테이션 Q3. Page Table이 너무 커지면 어떻게 해결하나요?\nA.\n다단계 페이지 테이블 TLB (Translation Lookaside Buffer): 캐시 역할 페이징+세그멘테이션 혼합 구조 "},{"id":6,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C/6-virtual-memory/","title":"6. 가상 메모리","section":"운영체제","content":" 1. 정의 # 가상 메모리(Virtual Memory) 는 프로세스가 실제 물리 메모리보다 더 큰 주소 공간을 사용할 수 있도록 지원하는 메커니즘\n논리 주소 ↔ 물리 주소 변환을 통해, 프로세스는 독립된 메모리 공간을 가진 것처럼 동작할 수 있음.\n목적 # 프로세스 간 메모리 격리 및 보호 실제 메모리보다 큰 공간 지원 (주소 추상화) 프로그램의 효율적 로딩 및 실행 (부분 적재, Demand Paging) 2. 주소 변환 흐름 (Page 기반) # 주소 구조\n[논리 주소] = [페이지 번호] + [페이지 오프셋] → MMU가 페이지 번호를 Page Table 통해 물리 메모리의 Frame으로 변환 → 최종 물리 주소 = Frame 시작 주소 + Offset e.g.\n가상 주소 : 0x1234 (4KB 페이지 크기) -\u0026gt; 페이지 번호 : 0x1, 오프셋 : 0x234 -\u0026gt; Page Table에서 Page 1이 Frame 5에 매핑되어 있으면 -\u0026gt; 물리 주소 = 5 * 4KB + 0x234 3. TLB(Translation Lookaside Buffer) # Page Table을 매번 조회하면 성능 저하 → TLB는 MMU 내부에 있는 캐시 최근 사용된 페이지 번호와 프레임 매핑 정보를 저장 상황 설명 TLB Hit TLB에서 바로 프레임 주소 조회 TLB Miss Page Table 조회 필요 → TLB 갱신 =\u0026gt; TLB는 매우 빠르지만 용량 제한 있음 -\u0026gt; LRU 기반 캐시 정책 적용\n4. Demand Paging # 실제 필요한 페이지만 메모리에 로드하는 방식 (→ 전체 프로그램을 처음부터 메모리에 올리지 않음)\n흐름 # 프로세스가 아직 로드되지 않은 페이지 접근 Page Fault 발생 (해당 페이지 없음) 디스크에서 해당 페이지 로드 Page Table 및 TLB 갱신 후 재시도 장점 : 메모리 절약, 빠른 프로세스 시작 단점 : Page Fault 시 디스크 접근 -\u0026gt; 심각한 성능 저하\n5. 페이지 교체 알고리즘 # FIFO (First-In First-Out) 가장 오래된 페이지를 제거 구현은 쉽지만, Belady\u0026rsquo;s anomaly 발생 가능 (페이지 수 ↑ → 오히려 Miss ↑) LRU (Least Recently Used) 가장 오랫동안 사용되지 않은 페이지 제거 현실적인 성능 우수 → 구현은 상대적으로 복잡 Optimal (OPT) 앞으로 가장 오랫동안 사용되지 않을 페이지 제거 이상적인 알고리즘 (실제로 구현 불가 → 벤치마크용) Clock 알고리즘 (Second Chance) LRU 근사 방식. 각 페이지에 reference bit 사용 원형 큐 순회하며 bit=0인 페이지 제거 알고리즘 특징 성능 구현 난이도 FIFO 단순 낮음 쉬움 LRU 실제 상황에 적합 좋음 어려움 (stack/counter 필요) OPT 최적 최고 불가능 Clock LRU 근사 중상 중간 6. Working Set \u0026amp; Thrashing # Working Set # 지역성을 기반으로 가장 많이 사용하는 페이지를 미리 저장해둔 것 일정 시간 동안 참조되는 페이지 집합 Thrashing # Working Set보다 할당된 메모리 프레임이 적을 때 발생 페이지 교체가 빈번히 일어나면서 시스템 전체 성능 저하 해결 # 프로세스 별 Working Set 기반 프레임 동적 할당 페이지 교체 알고리즘 개선 프로세스 수 조절 7. 실무 예시 # 시스템 가상 메모리 동작 Linux Demand Paging + Clock-based replacement (Clock-Pro) Windows Working Set 기반 프레임 관리 + Aging 정책 Docker/Container Namespace 및 CGroup 기반 메모리 격리, 가상 주소 사용 JVM Heap 영역이 논리 주소 → 실제 물리 할당은 GC와 OS가 결정 8. 자주 묻는 면접 질문 # Q1. 페이지 교체 알고리즘 중 가장 좋은 건?\nA.\n이론적으로는 OPT가 가장 우수하나 실제 구현 불가. LRU가 현실적으로 가장 균형 잡힘. Clock 알고리즘은 구현과 성능의 트레이드오프를 잘 반영한 방식. Q2. TLB Miss가 자주 발생하는 이유와 해결 방법은?\nA.\n프로세스 전환 시 TLB 내용이 무효화되기 때문 → Context Switch + Address Space Identifier(ASID) 사용으로 완화 가능. Q3. Thrashing이 발생하는 이유는?\nA. Working Set보다 작은 수의 프레임 할당 → 계속 페이지 교체 발생 → CPU보다 I/O가 바빠짐 → 시스템 정체\nQ4. Page Fault가 발생했을 때 처리 과정은?\nMMU가 Page Table 확인 → 없는 페이지 OS가 디스크에서 페이지 로딩 Page Table 갱신 TLB 갱신 프로세스 재시도 "},{"id":7,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C/7-file-system/","title":"7. File System","section":"운영체제","content":" 1. 파일 시스템의 역할 # 데이터를 저장하고, 탐색하고, 보호하는 계층 저장 장치(HDD, SSD 등)의 추상화 계층을 제공 운영체제는 파일 시스템을 통해 파일/디렉토리를 관리함 2. inode와 디렉터리 구조 # inode (Index Node) # 파일의 metadata를 저장하는 구조체 각 파일은 고유한 inode 번호를 가짐 (inode table에 저장) 포함 정보 예시 파일 크기 512KB 생성/수정/접근 시간 stat 명령으로 확인 접근 권한 rwxr-xr– 링크 수 hard link 개수 디스크 블록 위치 정보 직접/간접 블록 주소 포함 =\u0026gt; inode는 filename은 저장하지 않음 -\u0026gt; directory entry가 filename과 inode 번호를 매핑\n디렉터리 구조 # 디렉터리 = (파일명 \u0026lt;-\u0026gt; inode 번호)의 목록 계층적 트리 구조 (/, /home, /home/user/file.txt) 3. 파일 할당 방식 # 1) 연속 할당 (Continguous Allocation) # 연속된 디스크 블록에 파일 저장\n장점 단점 빠른 접근 파일 크기 변경 어려움, 외부 단편화 -\u0026gt; CD-ROM 등 읽기 전용 미디어에 적합\n2) 연결 할당 (Linked Allocation) # 각 블록이 다음 블록의 포인터를 포함 장점 단점 파일 크기 동적 변경 가능 임의 접근 속도 느림 (랜덤 액세스 불가) -\u0026gt; FAT (File Allocation Table)이 대표적인 예\n3) 인덱스 할당 (Indexed Allocation) # 별도의 인덱스 블록에 모든 데이터 블록 주소 저장 장점 단점 빠른 임의 접근 인덱스 블록의 크기 제한 있음 -\u0026gt; Unix 파일 시스템이 대표적 (inode 내부가 인덱스 역할)\nUnix inode 구조 - 직접/간접 블록 # 블록 설명 직접 블록 (0~11) 바로 데이터 블록 주소 단일 간접 블록 포인터 → 데이터 블록 주소들 이중 간접 블록 포인터 → 포인터 → 데이터 블록 삼중 간접 블록 포인터 ×3 → 매우 큰 파일 지원 📌 수백 MB ~ 수십 GB 크기 파일도 확장 가능\n4. 파일 접근 권한 및 접근 제어 # Unix 퍼미션 구조 (rwx) # 세 그룹으로 분리: 소유자 / 그룹 / 기타 사용자 각 그룹에 대해 read(r), write(w), execute(x) 권한 부여 예시 의미 -rwxr-xr\u0026ndash; 소유자: rwx, 그룹: r-x, 기타: r– chmod 755 file rwxr-xr-x 권한 설정 접근 제어 확장 (ACL, Capability) # ACL: 파일별로 사용자마다 세밀한 권한 설정 Capabilities: 커널에서 수행할 수 있는 동작 단위 권한화 5. 실무 예시 # 시스템 특징 ext4 (Linux) inode 기반, 저널링 지원, 성숙한 안정성 FAT32 (USB 등) 연결 할당 방식, 단순하지만 한계 있음 NTFS (Windows) 인덱스 + ACL 기반, 대용량/보안 지원 ZFS / Btrfs Copy-on-Write, 스냅샷, 체크섬 포함 6. 자주 묻는 면접 질문 # Q1. inode는 어떤 정보를 가지고 있나요?\nA.\n파일의 메타데이터(크기, 생성/수정 시각, 권한, 링크 수, 디스크 블록 위치)를 저장하며, 파일명은 포함되지 않습니다. 파일명은 디렉토리 엔트리에서 inode 번호와 매핑됩니다. Q2. 연속/연결/인덱스 할당의 차이를 설명해주세요.\nA.\n방식 접근 성능 파일 크기 유연성 단편화 연속 빠름 (임의 접근 가능) 낮음 외부 단편화 연결 느림 (순차 접근만) 높음 없음 인덱스 빠름 (임의 접근 가능) 높음 거의 없음 Q3. 대용량 파일을 inode로 어떻게 저장하나요?\nA.\nUnix inode는 직접 블록으로 시작하여, 단일 → 이중 → 삼중 간접 블록으로 확장할 수 있으며, 이를 통해 수십 GB까지의 대용량 파일을 관리할 수 있습니다. Q4. 파일 시스템에서 ‘파일명’은 어디에 저장되나요?\nA.\n디렉토리 엔트리에 저장됩니다. 디렉토리는 (파일명 → inode 번호) 매핑 테이블이며, 실제 파일 정보는 해당 inode에 있음. "},{"id":8,"href":"/docs/data-lake-platform/ceph/rbd-sc/","title":"Ceph RBD 설정하기(Block Storage)","section":"Ceph","content":" Ceph RBD를 Kubernetes PVC로 연동하기 # 순서는 다음과 같다.\nRBD용 Pool 생성 StorageClass 생성 PVC 생성 PVC를 사용하는 테스트 Pod 생성 정상 Mount 여부 확인 1. RDB용 Pool 생성 # kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash 로 rook-ceph-tools에 접속해서 아래 명령어를 수행한다.\nceph osd pool create {Pool명} {PG 수} PG란?\nPlacement Group의 약자 Ceph에서는 데이터를 곧바로 OSD에 저장하는 게 아닌, 중간에 PG를 거쳐 저장함 데이터를 여러 OSD에 분산 저장할 때 \u0026ldquo;분산의 단위\u0026rdquo; 역할을 함 데이터가 저장될 위치를 결정할 때, \u0026ldquo;Pool -\u0026gt; PG -\u0026gt; OSD\u0026quot;라는 경로를 통해 결정함 복제도 PG 단위로 관리함 구분 설명 OSD 실제 디스크에 저장하는 단위 (Object Storage Daemon) PG (Placement Group) 여러 OSD 사이에서 데이터를 배치(placement)하고 그룹핑하는 논리 단위 Object 사용자 데이터 (파일, 블록 등)를 쪼갠 실제 저장 단위 즉, 데이터의 흐름은 다음과 같다\nClient -\u0026gt; Pool -\u0026gt; Placement Group (PG) -\u0026gt; OSD PG 수 정하기 # PG 수는 OSD 개수와 Replication Factor(복제본 개수)를 고려해서 정해야한다. 기본적인 공식은 다음과 같다.\n$PG 수≈(OSD 개수)×100÷복제본 수 (replica)$\n현재 내 서버의 상황은 다음과 같다.\nOSD 3개 복제본(replica) 기본 3개 ~\u0026gt; 100개 정도가 권장됨\n-\u0026gt; 따라서 Ceph의 기본 권장치 중 하나인 128개로 선택함 PG 수는 너무 적으면 데이터 분산이 비효율적이고, 너무 많으면 OSD 관리 부하가 커진다. (3~5개 OSD로 시작할 땐 128 정도가 적당함)\n2. RBD StorageClass 생성하기 # Ceph POol(replicapool)을 k8s에서 쓸 수 있도록 StorageClass를 만들어야 한다.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-ceph-block provisioner: rook-ceph.rbd.csi.ceph.com parameters: clusterID: rook-ceph pool: replicapool imageFormat: \u0026#34;2\u0026#34; imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph reclaimPolicy: Delete allowVolumeExpansion: true 3. PVC + PVC를 사용하는 테스트 Pod 생성 # # rbd-pvc-test.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: rbd-pvc namespace: default spec: storageClassName: rook-ceph-block accessModes: - ReadWriteOnce resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: rbd-pod namespace: default spec: containers: - name: rbd-container image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 3600\u0026#34;] volumeMounts: - mountPath: \u0026#34;/mnt/rbd\u0026#34; name: rbd-vol volumes: - name: rbd-vol persistentVolumeClaim: claimName: rbd-pvc 4. 정상 Mount 여부 확인 # kubectl -n test exec -it rbd-pod -- sh # 내부 진입 후 ls /mnt/rbd # =\u0026gt; 정상적으로 디렉토리 접근 가능 "},{"id":9,"href":"/docs/data-lake-platform/ceph/ceph-installation/","title":"Ceph 개요 및 구축하기","section":"Ceph","content":" Ceph이란? # 완전히 분산되고, 확장성과 고가용성을 지향하는 storage platform\n핵심 목표\n단일 클러스터에서 Object Storage, Block Storage, File System 모두 제공 분산성, Self-Healing, 무중단 확장을 기본 설계 철학으로 둠 하드웨어에 종속되지 않고 x86 서버 + disk 조합으로 대규모 스토리지 시스템 구축 가능 주요 특징\nCRUSH 알고리즘(Controlled Replication Under Scalable Hashing) 기반 분산 데이터 맵핑. 별도의 중앙 메타데이터 서버 없이 데이터 위치 결정 (Object Storage). 장애 발생 시 자동 복구(Self Healing). 수평 확장(linear scalability) 가능 (노드 추가만으로 확장). 다양한 인터페이스 제공: S3 호환(Object), RBD(Block), CephFS(File System). Ceph의 구성요소 # 컴포넌트 설명 MON (Monitor) Ceph 클러스터의 맵 관리와 Quorum(합의)을 담당.\n- 모든 클러스터 상태(MAP) 저장\n- 3개 이상 추천 (항상 홀수) MGR (Manager) Ceph의 상태 모니터링, 모듈 관리 담당.\n- 대시보드, 모니터링 통합, Prometheus Exporter 제공\n- 1 active + 1 standby 형태로 구성 추천 OSD (Object Storage Daemon) 실제 데이터 저장.\n- 물리 디스크 하나당 1 OSD 프로세스\n- Ceph Pool 복제(replication), 데이터 복구, 리밸런싱 담당 MDS (Metadata Server) CephFS(파일시스템) 메타데이터 관리.\n- 블록/오브젝트는 필요 없음. CephFS 쓸 때만 필요 RGW (Rados Gateway) S3, Swift API를 제공하는 게이트웨이.\n- 오브젝트 스토리지 접속용 서비스 CSI Driver Kubernetes PVC를 Ceph RBD, CephFS와 연결하는 역할.\n- Rook Operator가 자동으로 설치 저장 방식 필요한 컴포넌트 Block Storage (RBD) MON, MGR, OSD Object Storage (S3) MON, MGR, OSD, RGW File Storage (CephFS) MON, MGR, OSD, MDS helm repo add {rook_repo명} https://charts.rook.io/release helm install {rook-ceph_chart명} rook/rook-ceph --version 1.17.0 Ceph 구축하기 # ceph-cluster.yaml apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph namespace: rook-ceph spec: cephVersion: image: quay.io/ceph/ceph:v19.2.2 allowUnsupported: false dataDirHostPath: /var/lib/rook skipUpgradeChecks: false continueUpgradeAfterChecksEvenIfNotHealthy: false mon: count: 3 allowMultiplePerNode: false mgr: count: 2 allowMultiplePerNode: false dashboard: enabled: true ssl: false monitoring: enabled: false network: connections: encryption: enabled: false compression: enabled: false requireMsgr2: false crashCollector: disable: false logCollector: enabled: true periodicity: daily maxLogSize: 500M cleanupPolicy: confirmation: \u0026#34;\u0026#34; sanitizeDisks: method: quick dataSource: zero iteration: 1 allowUninstallWithVolumes: false removeOSDsIfOutAndSafeToRemove: false priorityClassNames: mon: system-node-critical osd: system-node-critical mgr: system-cluster-critical storage: useAllNodes: true useAllDevices: true config: # 필요한 경우 여기에 추가 설정 가능 (ex. databaseSizeMB) allowDeviceClassUpdate: false allowOsdCrushWeightUpdate: false scheduleAlways: false onlyApplyOSDPlacement: false disruptionManagement: managePodBudgets: true osdMaintenanceTimeout: 30 csi: readAffinity: enabled: false cephfs: # fuseMountOptions, kernelMountOptions 설정 가능 (기본은 비워둠) healthCheck: daemonHealth: mon: disabled: false interval: 45s osd: disabled: false interval: 60s status: disabled: false interval: 60s livenessProbe: mon: disabled: false mgr: disabled: false osd: disabled: false startupProbe: mon: disabled: false mgr: disabled: false osd: disabled: false 좀 더 간단하게 하고싶다면,\nceph-cluster-short.yaml apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph namespace: rook-ceph spec: cephVersion: image: quay.io/ceph/ceph:v19.2.2 dataDirHostPath: /var/lib/rook mon: count: 3 allowMultiplePerNode: false dashboard: enabled: true ssl: false storage: useAllNodes: true useAllDevices: true config: {} disruptionManagement: managePodBudgets: true 하지만 현재 상황에서 리소스가 여유롭진 않으니,\nceph-cluster-limit.yaml apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph namespace: rook-ceph spec: cephVersion: image: quay.io/ceph/ceph:v18.2.2 allowUnsupported: false dataDirHostPath: /var/lib/rook mon: count: 3 allowMultiplePerNode: false mgr: count: 1 allowMultiplePerNode: false dashboard: enabled: true ssl: false crashCollector: # 🔻 비활성화 disable: true logCollector: enabled: true periodicity: daily maxLogSize: 500M storage: useAllNodes: true useAllDevices: false deviceFilter: \u0026#34;vdb\u0026#34; config: osdsPerDevice: \u0026#34;1\u0026#34; disruptionManagement: managePodBudgets: true osdMaintenanceTimeout: 30 priorityClassNames: mon: system-node-critical osd: system-node-critical mgr: system-cluster-critical healthCheck: daemonHealth: mon: disabled: false interval: 45s osd: disabled: false interval: 60s status: disabled: false interval: 60s resources: # 🔧 각 데몬별 리소스 제한 mgr: limits: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;256Mi\u0026#34; requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;128Mi\u0026#34; mon: limits: cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;512Mi\u0026#34; requests: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;256Mi\u0026#34; osd: limits: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; requests: cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;1Gi\u0026#34; 적용 포인트 # 항목 설정 내용 Namespace rook-ceph Ceph Version v19.2.2 (Squid 최신 안정판) Mon Count 3개 Mgr Count 2개 (Active + Standby) OSD 구성 useAllDevices: true (vdb 자동 인식) Dashboard 활성화, SSL 끄기 (HTTP) Disk Encryption 사용 안 함 (내부망 고려) Monitoring 비활성화 (Prometheus 아직 설치 안 되어 있음) CrashCollector/LogCollector 활성화 Disruption Management 활성화 (PodDisruptionBudget 적용) CleanUp Policy 기본값 (데이터 보존) Future Expansion Worker, OSD 추가 용이하게 설정 \u0026ldquo;If you want all nodes and all available devices to be used automatically, set useAllNodes: true and useAllDevices: true. Specifying nodes: list is only necessary when you want fine-grained control over device selection.\u0026rdquo;\u0026quot;\n현재 worker 3,4,5만 vdb가 추가되어있어서 자동으로 w3,w4,w5에만 OSD가 생성됨 추가로 w6, w7에 vdb 추가하면 자동으로 OSD도 확장됨 항목 일반 설정 이유 skipUpgradeChecks: false 업그레이드 시 cluster health 체크함 (안정성) allowMultiplePerNode: false 같은 노드에 MON 2개 생성 방지 (HA 보장) dashboard.ssl: false 내부망이면 SSL 불필요, 운영 편의성 useAllDevices: true 자동 디바이스 인식, 디스크 추가시 확장성 managePodBudgets: true PodDisruptionBudget 적용, 장애/노드Drain 시 안정성 logCollector: enabled: true 장애 분석 위해 daemon 로그 파일 저장 crashCollector: disable: false crash 발생 시 crash report 수집 healthCheck: 활성화 MON, OSD, MGR 주기적으로 상태 점검 "},{"id":10,"href":"/docs/data-lake-platform/ceph/ceph-install-guide-short/","title":"Ceph 구축하기 - 요약본","section":"Ceph","content":" 1. Rook Operator 배포 # # 공식 Helm 차트로 설치 helm repo add rook-release https://charts.rook.io/release helm repo update # 네임스페이스 생성 kubectl create ns rook-ceph # Operator 설치 helm install rook-ceph rook-release/rook-ceph --namespace rook-ceph 2. CephCluster 리소스 배포 # ceph-cluster.yaml apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph namespace: rook-ceph spec: cephVersion: image: quay.io/ceph/ceph:v18.2.2 allowUnsupported: false dataDirHostPath: /var/lib/rook mon: count: 3 allowMultiplePerNode: false mgr: count: 1 allowMultiplePerNode: false dashboard: enabled: true ssl: false crashCollector: disable: true logCollector: enabled: true periodicity: daily maxLogSize: 500M storage: useAllNodes: true useAllDevices: false deviceFilter: \u0026#34;vdb\u0026#34; config: osdsPerDevice: \u0026#34;1\u0026#34; disruptionManagement: managePodBudgets: true osdMaintenanceTimeout: 30 priorityClassNames: mon: system-node-critical osd: system-node-critical mgr: system-cluster-critical healthCheck: daemonHealth: mon: disabled: false interval: 45s osd: disabled: false interval: 60s status: disabled: false interval: 60s resources: # 🔧 각 데몬별 리소스 제한 mgr: limits: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;256Mi\u0026#34; requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;128Mi\u0026#34; mon: limits: cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;512Mi\u0026#34; requests: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;256Mi\u0026#34; osd: limits: cpu: \u0026#34;1000m\u0026#34; memory: \u0026#34;2Gi\u0026#34; requests: cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;1Gi\u0026#34; k apply -f ceph-cluster.yaml를 실행해 리소스를 배포한다.\nmon, mgr, osd 개수와 설정 deviceFilter로 사용할 디스크 선택 (vdb 등) 리소스 제한 health check, priority 설정 등 3. RBD를 위한 CephBlockPool \u0026amp; StorageClasss 구성 # CephBlockPool.yaml\napiVersion: ceph.rook.io/v1 kind: CephBlockPool metadata: name: replicapool namespace: rook-ceph spec: failureDomain: host replicated: size: 3 StorageClass.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-ceph-block provisioner: rook-ceph.rbd.csi.ceph.com parameters: clusterID: rook-ceph pool: replicapool imageFormat: \u0026#34;2\u0026#34; imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard 위 sc를 기준으로 pvc를 생성하면 자동으로 RBD 볼륨이 생성됨.\n"},{"id":11,"href":"/docs/data-lake-platform/ceph/ceph-tools-pod/","title":"Ceph 초기 설정하기","section":"Ceph","content":"ceph가 정상적으로 설치됐다면 다음의 설정들을 추가해준다. ceph-tools는 운영자가 다루는 pod라서 따로 추가해줘야한다.\nrook-ceph-config.yaml\napiVersion: v1 kind: ConfigMap metadata: name: rook-ceph-config namespace: rook-ceph data: ceph.conf: | [global] fsid = cb669020-44b5-4528-bdf8-5f7e082cbf71 mon_host = 10.233.41.119,10.233.10.62,10.233.45.253 mon_initial_members = a,b,c auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx log_to_stderr = false err_to_stderr = false log_to_syslog = false log_to_monitors = true fsid 값과 mon_host IP는 각 클러스터 환경에 맞게 수정 필요. rook-ceph-admin-keyring.yaml\napiVersion: v1 kind: Secret metadata: name: rook-ceph-admin-keyring namespace: rook-ceph type: Opaque stringData: ceph.client.admin.keyring: | [client.admin] key = AQAH+wFonT6EHhAAgGph5NeWPyTTO41HkFOpDg== caps mds = \u0026#34;allow *\u0026#34; caps mgr = \u0026#34;allow *\u0026#34; caps mon = \u0026#34;allow *\u0026#34; caps osd = \u0026#34;allow *\u0026#34; key = ~ 값은 Ceph Dashboard → Admin 계정 Export로 가져온 값 사용. 주의: stringData를 써야 Base64 인코딩 없이 바로 적용할 수 있음.\n=\u0026gt; key 이름 주의: ceph.client.admin.keyring 이 정확히 필요함. rook-ceph-tools.yaml apiVersion: apps/v1 kind: Deployment metadata: name: rook-ceph-tools namespace: rook-ceph spec: replicas: 1 selector: matchLabels: app: rook-ceph-tools template: metadata: labels: app: rook-ceph-tools spec: containers: - name: rook-ceph-tools image: rook/ceph:v1.12.10 command: [\u0026#34;/usr/bin/bash\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;sleep infinity\u0026#34;] imagePullPolicy: IfNotPresent securityContext: privileged: true volumeMounts: - name: ceph-conf mountPath: /etc/ceph volumes: - name: ceph-conf projected: sources: - configMap: name: rook-ceph-config items: - key: ceph.conf path: ceph.conf - secret: name: rook-ceph-admin-keyring items: - key: ceph.client.admin.keyring path: ceph.client.admin.keyring 정상 동작 확인 방법\nkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash 에 들어가서\ncat /etc/ceph/ceph.conf cat /etc/ceph/ceph.client.admin.keyring ceph status 항목 설명 ConfigMap /etc/ceph/ceph.conf 파일을 주입하기 위함 Secret /etc/ceph/ceph.client.admin.keyring 파일을 주입하기 위함 rook-ceph-tools Deployment Toolbox Pod 자체, 위 ConfigMap/Secret을 /etc/ceph에 mount Troubleshooting # 문제 원인 해결 방법 RADOS object not found ceph.conf, keyring 둘 다 없었음 ConfigMap, Secret 주입 Malformed input keyring 포맷 문제 (key 값 틀림) Dashboard에서 client.admin 내보내기로 key 추출 후 적용 FailedMount 에러 key 이름 mismatch (key 대신 ceph.client.admin.keyring) Secret 안에 key 이름 수정 Toolbox ceph status 실패 config, keyring 둘 중 하나라도 잘못 mount됨 둘 다 제대로 적용 후 성공 "},{"id":12,"href":"/docs/data-lake-platform/ceph/ceph-fs/","title":"CephFS 구축하기","section":"Ceph","content":" 1. CephFS Filesystem 생성 # rook-ceph-filesystem.yaml\napiVersion: ceph.rook.io/v1 kind: CephFilesystem metadata: name: rook-ceph-fs namespace: rook-ceph spec: metadataPool: replicated: size: 3 dataPools: - replicated: size: 3 preserveFilesystemOnDelete: true metadataServer: activeCount: 1 activeStandby: true metadataPool, dataPools 모두 replication 3개로 설정(3 OSD) MDS(Metadata Server)는 1 active + 1 standby (HA) 2. CephFS용 StorageClass 생성 # rook-cephfs-sc.yaml\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-cephfs provisioner: rook-ceph.cephfs.csi.ceph.com parameters: clusterID: rook-ceph fsName: rook-ceph-fs pool: rook-ceph-fs-data0 csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - noatime CephFS는 rook-ceph-fs Filesystem을 바라봄 Ceph CSI가 자동으로 PV/PVC를 생성하고 Mount하도록 설정함 noatime 설정으로 파일 접근 시간 기록을 방지하여 성능 최적화 3. CephFS PVC + Pod 테스트 # cephfs-pvc-test.yaml\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: cephfs-pvc namespace: test spec: storageClassName: rook-cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: cephfs-pod namespace: test spec: containers: - name: cephfs-container image: busybox command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 3600\u0026#34;] volumeMounts: - mountPath: \u0026#34;/mnt/cephfs\u0026#34; name: cephfs-vol volumes: - name: cephfs-vol persistentVolumeClaim: claimName: cephfs-pvc CephFS는 기본적으로 RWX(ReadWriteMany)를 지원함. (여러 Pod에서 동시에 Mount 가능) 삭제 # cephFS는 RGW를 씀으로써 내 환경에서 너무 과하게 많은 리소스를 사용함.\nblock storage : Ceph RBD object storage : Minio File System : NFS\n위와 같이 사용함으로써 리소스 낭비를 줄이는 방향으로 간다. # 1. test 네임스페이스에서 cephfs-pvc, cephfs-pod 삭제 kubectl delete pvc cephfs-pvc -n test kubectl delete pod cephfs-pod -n test # 2. rook-cephfs StorageClass 삭제 kubectl delete sc rook-cephfs # 3. rook-ceph-fs CephFilesystem 삭제 kubectl delete cephfilesystem rook-ceph-fs -n rook-ceph # 4. (자동) MDS 관련 Deployment 삭제 확인 kubectl get deploy -n rook-ceph | grep mds # 5. rook-ceph-fs 관련 Ceph Pools 삭제 kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash # rook-ceph-tools 내부에서 실행 ceph osd pool ls ceph osd pool delete rook-ceph-fs-metadata rook-ceph-fs-metadata --yes-i-really-really-mean-it ceph osd pool delete rook-ceph-fs-data0 rook-ceph-fs-data0 --yes-i-really-really-mean-it # 삭제 후 cluster 상태 점검 ceph status "},{"id":13,"href":"/docs/troubleshooting/ceph-issue1/","title":"Ceph를 구축하며 발생한 cpu 이슈..","section":"트러블슈팅","content":" 상황 # Ceph 클러스터를 Kubernetes에 구축하는 과정 중, csi-rbdplugin, csi-cephfsplugin, csi-snapshotter 등의 csi-* 계열 Pod들이 CrashLoopBackOff 또는 Error 상태로 지속적으로 실패함.\n문제 원인 # Ceph과 glibc의 관계 Ceph은 내부적으로 glibc(GNU C Library)를 사용하여 시스템 호출과 메모리 관리를 수행함. 최신 Ceph(v18, v19) 버전은, 최신 glibc(\u0026gt;=2.34) 기반으로 빌드되어 있음. 이 glibc 버전들은 CPU가 \u0026lsquo;x86-64-v2\u0026rsquo; 명령어 집합을 지원한다고 가정하고 최적화되어 있음. x86-64-v2란? x86-64-v2는 기존 x86-64 아키텍처보다 더 발전된 명령어 집합(Instruction Set)을 의미함.\n필수 지원 명령어는 다음과 같음:\n플래그 설명 sse3 스트리밍 SIMD 확장(SSE3) ssse3 보강된 스트리밍 SIMD 확장(SSSE3) sse4_1 스트리밍 SIMD 확장(SSE4.1) sse4_2 스트리밍 SIMD 확장(SSE4.2) popcnt 비트 카운트(Population Count) cx16 16바이트 비교/교환(CMPXCHG16B) 이 중 하나라도 CPU가 지원하지 않으면 glibc가 오류를 발생시키고, Ceph 데몬이 프로세스 레벨에서 바로 죽음.\n가상화 환경의 문제 KVM/QEMU 환경에서는 기본적으로 \u0026ldquo;QEMU Virtual CPU\u0026quot;라는 에뮬레이션 CPU를 VM에게 제공함. 이 기본 설정은 x86-64-v2 명령어 지원이 없음. 따라서 Ceph 데몬(특히 mon, mgr, osd 등)이 초기화될 때, 필수 명령어를 찾지 못해 에러 발생 → Pod Crash로 이어짐. 확인 방법 # VM 또는 Kubernetes Node에서 다음 명령어 실행\nlscpu | grep Flags # or cat /proc/cpuinfo | grep flags 출력 결과에 sse3, ssse3, sse4_1, sse4_2, popcnt, cx16 등이 반드시 포함되어 있어야 함. 만약 포함되지 않았다면, 해당 VM은 x86-64-v2 기준을 만족하지 않음. 해결 방법 # 항목 설정값 이유 CPU Type Host Passthrough 물리 CPU의 모든 기능을 VM에 직접 전달하여 x86-64-v2 명령어 지원 보장 Emulated Machine 기존과 동일 (예: pc-q35-6.2) 별도 변경 불필요 디스크 포맷 qcow2 성능과 스냅샷 기능을 동시에 고려 부팅 디스크 30GB ~ 50GB Kubernetes OS용 디스크 사이즈 추가 디스크 필요시 추가 Ceph OSD 디스크용 등 Firmware BIOS 기존 환경 유지 vCPU 계획한 대로 (ex: master 3core, worker 4core) Memory 계획한 대로 (ex: master 8GB, worker 16GB) 네트워크 Bridge 연결 (br0 등) Kubernetes 클러스터 통신을 위해 요약 # QEMU 기본 CPU → x86-64-v2 명령어 미지원 → 최신 glibc 기반 Ceph 데몬 죽음 → csi-*, mon, mgr Pod Crash. → VM 생성 시 Host Passthrough 설정 필수!\n"},{"id":14,"href":"/docs/data-engineering/modeling/dimensional-modeling/","title":"Dimensional Modeling: 분석을 위한 데이터 설계의 본질","section":"Modeling","content":" Fact와 Dimension의 개념 # Fact : \u0026ldquo;측정값 / 수치 중심의 이벤트 데이터\u0026rdquo;\n일반적으로는 집계(Aggregation) 가 가능한 값들 대부분 시간에 따라 변화하며, 특정 기준으로 분류하여 분석하는 대상 항상 하나 이상의 Dimension과 연결되어 있으며, 분석 쿼리에서는 집계 함수(SUM, COUNT, AVG 등)의 대상이 됨 sales_fact ├── date_id (FK) ├── product_id (FK) ├── store_id (FK) ├── quantity_sold (Fact) ├── total_revenue (Fact) Dimension : \u0026ldquo;Fact를 설명하거나 분석 기준이 되는 범주형 데이터\u0026rdquo;\n일반적으로 분석자가 데이터를 \u0026ldquo;어떤 기준으로 보고 싶은가\u0026quot;를 정의하는 기준 WHERE, GROUP BY, ORDER BY 절에서 자주 사용됨 각 Dimension은 여러 속성(attribute)를 가질 수 있고, 이 속성 간에는 종종 계층(Hierarchy)이 존재함 예를 들어, 시간 Dimesion은 연도 -\u0026gt; 분기 -\u0026gt; 월 -\u0026gt; 일, 지역 Dimension은 대륙 -\u0026gt; 국가 -\u0026gt; 도시 순으로 계층화할 수 있음 e.g. 카테고리, 지역, 시간 고객, 상품 등의 범주적(categorical) 데이터 product_dim ├── product_id (PK) ├── product_name ├── category ├── brand Fact는 수치 결과, Dimension은 분석 기준\nDimensional Modeling이란? # 분석을 위해 데이터를 Fact + Dimension 구조로 설계하는 방법론\nRDB 기반 DW 구조 설계에 최적화됨 Star / Snowflake / Galaxy 구조로 구현됨 분석 쿼리를 효율적으로 처리하기 위한 구조적 기반 구성 요소 # Fact Table: 수치 중심, 여러 Dimension에 연결됨 Dimension Table: 분석 기준 제공, 계층 포함 가능 Grain: 하나의 Fact가 의미하는 단위 (ex. 1건의 주문? 1일 매출?) Multidimensional Modeing # Multidimensional Modeling은 데이터를 다양한 분석 축(Dimension)으로 나누어 보고, 각 축의 조합에 따라 하나의 측정값(Fact)를 도출하는 방식임. OLAP의 기본 모델링 방식으로, 사용자는 다양한 축을 기준으로 데이터를 Slice, Dice, Drill-down, Roll-up 등의 방식으로 분석 가능. 이 모델은 특히 OLAP Cube라는 개념으로 시각화되며, X, Y, Z 축에 Dimension이 위치하고, 교차점(Cell)에 하나의 Fact 값이 존재. 분석 중심으로 최적화되어 있으며, 빠른 응답 성능과 사용자의 직관적인 이해를 위해 설계됨. 시간 차원 ^ | | ● (제품A, 2023년, 서울, 100만원) | / | / | / | / |/ +------------\u0026gt; 제품 차원 /| / | / | / | / | v v 지역 차원 측정값(판매액) Multidimensional Modeling과의 관계 # 항목 Dimensional Modeling Multidimensional Modeling 관점 구조적 설계 방법론 개념적 분석 모델링 구현 Star/Snowflake Schema OLAP Cube 기반 축 해석 사용 RDB 기반 DW 설계 BI/OLAP 분석 툴 설계 둘은 목적이 다르지만 실무에서는 거의 연동되어 사용된다고 함 Cube로 보는 시각은 Multidimensional, 테이블 설계는 Dimensional Star, Snowflake Schema # ⭐ Star Schema # 비정규화된 Dimension 테이블 + 중심 Fact 테이블 쿼리 단순, JOIN 적음 BI 도구에서 선호됨 [product_dim] ↑ [store_dim] ← [sales_fact] → [date_dim] ❄️ Snowflake Schema # Dimension을 정규화하여 계층 구조 표현 중복 적고, 정합성 높음 JOIN이 많아져 성능 저하 가능 category_dim ← subcategory_dim ← product_dim ← sales_fact 🌌 Galaxy Schema # 여러 Fact 테이블 존재 공통 Dimension을 공유 (통합 분석용) 구조 복잡하고 설계 난이도 높음 [product_dim] ↑ ↑ [sales_fact] [refund_fact]` 스키마 구조 복잡도 성능 정합성 특징 Star 낮음 빠름 중간 단순 분석에 최적화 Snowflake 중간 느릴 수 있음 높음 계층 표현에 유리 Galaxy 높음 복합 높음 통합 분석 + 확장성 스키마 선택 기준 # 데이터 중복을 감수하더라도 분석 편의성/속도가 우선 → ⭐ Star 계층 구조 표현, 정합성 유지가 중요 → ❄️ Snowflake 여러 이벤트 테이블(주문, 반품, 배송 등)을 함께 분석 → 🌌 Galaxy 💬 핵심: 구조는 도메인보다 \u0026ldquo;분석 목적 + 유지 전략\u0026rdquo; 기준으로 선택함\n"},{"id":15,"href":"/docs/data-processing/flink/flink-to-gcs/","title":"Flink to GCS","section":"Flink","content":" 1. GCP 준비 # Bucket 생성 Service account 생성 (최소 권한) Storage Object Admin Storage Object Creator Service account key(Json file) 생성 후 다운로드 2. K8s Secret 생성 # kubectl create secret generic {secret_key_name} \\ --from-file={1번에서 발급받은 json파일}={/path/to/your/keyfile.json} \\ -n flink # e.g. kubectl create secret generic gcp-steve-account-key \\ --from-file=squidengineer-25d36a46ab7f.json=/path/to/your/keyfile.json \\ -n flink 3. GCS Connector 준비 # 필요 JAR 목록 # flink-gs-fs-hadoop-1.20.1.jar : Flink가 GCS를 지원하는 플러그인 gcs-connector-hadoop2-latest.jar : GCS와 HDFS 인터페이스를 연결하는 Google 제공 connector wget https://repo1.maven.org/maven2/org/apache/flink/flink-gs-fs-hadoop/1.20.1/flink-gs-fs-hadoop-1.20.1.jar wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar 참고:\nflink-gs-fs-hadoop은 Flink 1.20 공식 릴리스부터 기본 지원.\n단, 여전히 \u0026ldquo;gcs-connector-hadoop\u0026quot;은 별도 필요함 (GCP 라이브러리).\n4. Custom Flink Docker image 만들기 # Dockerfile 작성 # FROM flink:1.20.1-scala_2.12 # GCS connector 플러그인 및 라이브러리 추가 RUN mkdir -p /opt/flink/plugins/flink-gs-fs-hadoop COPY flink-gs-fs-hadoop-1.20.1.jar /opt/flink/plugins/flink-gs-fs-hadoop/ COPY gcs-connector-hadoop2-latest.jar /opt/flink/lib/ # 플러그인 활성화 RUN echo \u0026#34;plugins.enable: flink-gs-fs-hadoop\u0026#34; \u0026gt;\u0026gt; /opt/flink/conf/flink-conf.yaml RUN chmod -R 755 /opt/flink/plugins/flink-gs-fs-hadoop 이미지 빌드 및 업로드 # docker build -t {dockerhub-username}/steve-flink-gcs:1.20.1 . docker push {dockerhub-username}/steve-flink-gcs:1.20.1 5. FlinkDeployment YAML 작성 # apiVersion: flink.apache.org/v1beta1 kind: FlinkDeployment metadata: name: {cluster-name} namespace: flink spec: image: {dockerhub-username}/steve-flink-gcs:1.20.1 flinkVersion: v1_20 serviceAccount: flink mode: standalone flinkConfiguration: taskmanager.numberOfTaskSlots: \u0026#34;2\u0026#34; jobmanager.memory.process.size: \u0026#34;1024m\u0026#34; taskmanager.memory.process.size: \u0026#34;2048m\u0026#34; kubernetes.rest-service.exposed.type: ClusterIP kubernetes.deployment.target: standalone fs.gs.project.id: {gcp-project-id} fs.gs.system.bucket: {bucket-name} fs.gs.auth.service.account.json.keyfile: /opt/flink/custom-config/{gcp-secret-key} fs.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem fs.AbstractFileSystem.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS plugins.enable: flink-gs-fs-hadoop jobManager: replicas: 1 resource: memory: \u0026#34;2048m\u0026#34; cpu: 1 podTemplate: spec: volumes: - name: gcp-sa-key secret: secretName: {secret-name} containers: - name: flink-main-container env: - name: GOOGLE_APPLICATION_CREDENTIALS value: /opt/flink/custom-config/{gcp-secret-key} volumeMounts: - mountPath: /opt/flink/custom-config/{gcp-secret-key} name: gcp-sa-key subPath: {gcp-secret-key} taskManager: replicas: 2 resource: memory: \u0026#34;2048m\u0026#34; cpu: 1 podTemplate: spec: volumes: - name: gcp-sa-key secret: secretName: {secret-name} containers: - name: flink-main-container volumeMounts: - mountPath: /opt/flink/custom-config/{gcp-secret-key} name: gcp-sa-key subPath: {gcp-secret-key} env: - name: GOOGLE_APPLICATION_CREDENTIALS value: /opt/flink/custom-config/squidengineer-25d36a46ab7f.json ingress: template: \u0026#34;{ingress-name}\u0026#34; className: nginx annotations: nginx.ingress.kubernetes.io/backend-protocol: \u0026#34;HTTP\u0026#34; nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; cert-manager.io/cluster-issuer: \u0026#34;selfsigned-cluster-issuer\u0026#34; 꼭 env로 GOOGLE_APPLICATION_CREDENTIALS 설정을 해줘야 권한 인증 가능\n6. Flink SQL Client로 데이터 삽입 # kubectl exec -it -n flink deploy/{cluster-name}-jobmanager -- /opt/flink/bin/sql-client.sh embedded 로 접속 후, 테스트 용도로\nDROP TABLE IF EXISTS {TABLE_NAME}; CREATE TABLE {TABLE_NAME} ( id INT, name STRING ) WITH ( \u0026#39;connector\u0026#39; = \u0026#39;filesystem\u0026#39;, \u0026#39;path\u0026#39; = \u0026#39;gs://{bucket-name}/{folder-name}/\u0026#39;, \u0026#39;format\u0026#39; = \u0026#39;csv\u0026#39;, \u0026#39;sink.partition-commit.policy.kind\u0026#39; = \u0026#39;success-file\u0026#39; ); INSERT INTO {TABLE_NAME} VALUES (1, \u0026#39;steve\u0026#39;), (2, \u0026#39;chatgpt\u0026#39;), (3, \u0026#39;gpt4o\u0026#39;); 위를 실행한다.\n"},{"id":16,"href":"/docs/infra/kvm/vm-metadata/","title":"Hello","section":"KVM","content":"인프라 구성은 다음과 같다.\nHome(192.168.0.19) # VM명 Disk CPU RAM Disk 사이즈 ctrl-node /mnt/kvm-pool/ctrl-node.qcow2 2 2GB 40GB k8s-m1 /mnt/kvm-pool/k8s-m1.qcow2 3 8GB 30GB k8s-m2 /mnt/kvm-pool/k8s-m2.qcow2 3 8GB 30GB k8s-w1 /mnt/kvm-pool/k8s-w1.qcow2 3 16GB 50GB + Ceph 100GB k8s-w2 /mnt/kvm-pool/k8s-w2.qcow2 4 16GB 50GB + Ceph 100GB 총합 15 50GB Test(192.168.0.18) # VM명 Disk CPU RAM Disk 사이즈 k8s-m3 /mnt/kvm-pool/k8s-m3.qcow2 4 8GB 30GB k8s-w3 /mnt/kvm-pool/k8s-w3.qcow2 4 16GB 50GB + Ceph 100GB k8s-w4 /mnt/kvm-pool/k8s-w4.qcow2 4 16GB 50GB + Ceph 100GB k8s-w5 /mnt/kvm-pool/k8s-w5.qcow2 4 16GB 50GB + Ceph 100GB nfs-server /mnt/kvm-pool/nfs.qcow2 1 2GB 30GB + NFS 300GB 총합 17 58GB IP # Key Value Subnet 192.168.0.0/24 Gateway 192.168.0.1 Name Servers 8.8.8.8, 1.1.1.1 Search Domains (비워도 무방) VM 이름 Address 입력값(고정 IP) 생성 여부 ctrl-node 192.168.0.100/24 O k8s-m1 192.168.0.101/24 O k8s-m2 192.168.0.102/24 O k8s-m3 192.168.0.103/24 O worker k8s-w1 192.168.0.111/24 O k8s-w2 192.168.0.112/24 O k8s-w3 192.168.0.113/24 O k8s-w4 192.168.0.114/24 O k8s-w5 192.168.0.115/24 O nfs-server 192.168.0.200/24 O GPU있는 서버(192.168.0.19) # VM명 Disk CPU RAM Disk 사이즈 k8s-w3 4 20 k8s-w4 4 18 k8w-w5 4 18 총합||16vcpu|62gb|\nMain(192.168.0.62) # VM명 Disk CPU RAM Disk 사이즈 k8s-m1 6 12GB 50GB k8s-w1 6 20GB k8s-w2 6 20GB IP # Key Value Subnet 192.168.0.0/24 Gateway 192.168.0.1 Name Servers 8.8.8.8, 1.1.1.1 Search Domains (비워도 무방) VM 이름 Address 입력값(고정 IP) 생성 여부 ctrl-node 192.168.0.100/24 O k8s-m1 192.168.0.101/24 O k8s-m2 192.168.0.102/24 O k8s-m3 192.168.0.103/24 O worker k8s-w1 192.168.0.111/24 O k8s-w2 192.168.0.112/24 O k8s-w3 192.168.0.113/24 O k8s-w4 192.168.0.114/24 O k8s-w5 192.168.0.115/24 O nfs-server 192.168.0.200/24 O "},{"id":17,"href":"/docs/data-integration/kafka/kafka-basic/","title":"Kafka 심화 개념","section":"Kafka","content":" Producer Acks, Batch, Page Cache and Flush # acks 설정은 요청이 성공할 때를 정의하는 데 사용되는 Producer에 설정하는 Parameter\nacks=0 : ack가 필요하지 않음. 이 수준은 자주 사용되지 않음. 메시지 손실이 다소 있더라도 빠르게 메시지를 보내야 하는 경우에 사용된다. acks=1(default) : Leader가 메시지를 수신하면 ack를 보냄. Leader가 Producer에게 ACK를 보낸 후, Follower가 복제하기념전에 Leader에 장애가 발생하면 메시지가 손실. “At most once(최대 한 번)” 전송을 보장 acks=-1(acks=all) : 메시지가 Leader가 모든 Replica까지 Commit 되면 ack를 보냄 Leader를 잃어도 데이터가 살아남을 수 있도록 보장. 그러나 대기 시간이 더 길고 특정 실패 사례에서 반복되는 데이터 발생 가능성 있음. “At least once(최소 한 번)” 전송을 보장 Producer Retry # retry와 관련된 Parameters:\nParameter 설명 Default 값 retries 메세지를 send하기 위해 재시도하는 횟수 MAX_INT retry.backoff.ms 재시도 사이에 추가되는 대기 시간 100 request.timeout.ms Producer가 응답을 기다리는 최대 시간 30,000(30초) delivery.timeout.ms send() 후 성공 또는 실패를 보고하는 시간의 상한 120,000(2분) =\u0026gt; retries를 조정하는 대신 delivery.timeout.ms 조정으로 retries 제어 Producer Batch 처리 # 메시지를 모아서 한번에 전송 Batch 처리는 RPC(Remote Procedure Call)수를 줄여서 Broker가 처리하는 작업이 줄어들기 때문에 더 나은 처리량을 제공 linger.ms : (default : 0). 메시지가 함께 Batch 처리될 때까지 대기 시간 batch.size : (default : 16KB). 보내기 전 Batch의 최대 크기 Batch 처리의 일반적인 설정은 linger.ms=100 및 batch.size=1000000\nPage Cache와 Flush # 메시지는 Partition에 기록됨 Partition은 Log Segment file로 구성 (기본값 : 1GB마다 새로운 Segment 생성) 성능을 위해 Log Segment는 OS Page Cache에 기록됨 로그 파일에 저장된 메시지의 데이터 형식은 Broker가 Producer로부터 수신한 것, 그리고 Consumer에게 보내는 것과 정확히 동일하므로, Zero-Copy(Zero-copy 전송은 데이터가, User Space에 복사되지 않고, CPU 개입 없이 Page Cache와 Network Buffer 사이에서 직접 전송되는 것을 의미. 이것을 통해 Broker Heap 메모리를 절약하고 또한 엄청난 처리량을 제공)가 가능 Page Cache는 다음과 같은 경우 디스크로 Flush됨 Broker가 완전히 종료 OS background “Flusher Thread” 실행 Flush 전에 Broker 장애가 발생하면\u0026hellip; # OS가 데이터를 디스크로 flush하기 전 broker의 시스템에 장애가 발생하면 해당 데이터가 손실됨 Partition이 Replication되어 있다면, Broker가 다시 온라인 상태가 되면 필요시 Leader Replica에서 데이터가 복구됨 Replication이 없다면, 데이터는 영구적 손실 가능 Kafka 자체 Flush 정책 # 마지막 Flush 이후의 메시지 수(log.flush.interval.messages) 또는 시간(log.flush.interval.ms)으로 Flush(fsync)를 트리거하도록 설정할 수 있음 Kafka는 운영 체제의 background Flush 기능(예: pdflush)을 더 효율적으로 허용하는 것을 선호하기 때문에 이러한 설정은 기본적으로 무한(기본적으로 fsync 비활성화)으로 설정 이러한 설정을 기본값으로 유지하는 것을 권장 *.log 파일을 보면 디스크로 Flush된 데이터와 아직 Flush되지 않은 Page Cache (OS Buffer)에 있는 데이터가 모두 표시됨 Flush된 항목과 Flush되지 않은 항목을 표시하는 Linux 도구(예: vmtouch)도 있음 Replica Failure # Replica Recovery # Consumer Rebalance # Partition Assignment Strategy # Cooperative Sticky Assignor # Kafka Log File # Log Retention and Cleanup Policy # Exactly Once Semantics(EOS) # "},{"id":18,"href":"/docs/data-integration/kafka/kafka-troubleshooting-optimization/","title":"Kafka 운영 요소","section":"Kafka","content":" Kafka Monitoring # Troubleshooting Zookeeper # Troubleshooting Broker # Troubleshooting Producer # Troubleshooting Consumer # Tuning Broker # Tuning Producer # Tuning Consumer # "},{"id":19,"href":"/docs/data-integration/kafka/kafka-data-structure/","title":"Kafka의 데이터 구조","section":"Kafka","content":"Kafka의 메세지 저장 구조는 논리적 구조와 물리적 구조로 나눠서 생각하는 것이 좋다. Kafka는 단순한 message queue를 넘어서 Log-structured Storage로 설계되어, 메세지의 저장 및 조회 방식이 일반적인 RDBMS나 단순 queueing system과는 다르다.\n논리적 구조 # Kafka Cluster └─ Topic └─ Partition (복수 개 가능) └─ Offset 기반 메시지 로그 (Append-Only) 🔹 Topic\n논리적인 메시지 스트림 단위 (예: sensor-temperature, chat-logs 등) 실제 저장은 partition 단위로 이루어짐 🔹 Partition\n메시지의 병렬 처리 단위 각 파티션은 내부적으로 append-only log로 구현됨 각 메시지는 Offset을 기준으로 순차적으로 저장됨 하나의 파티션은 한 번에 하나의 consumer에게만 할당됨 (Consumer Group 기준) 물리적 구조 # /log.dirs/\u0026lt;topic\u0026gt;-\u0026lt;partition\u0026gt;/ ├─ 00000000000000000000.log ├─ 00000000000000000000.index ├─ 00000000000000000000.timeindex ├─ 00000000000000010000.log ├─ 00000000000000010000.index ├─ ... 🔹 Segment (세그먼트)\n파티션은 실제로 여러 개의 segment 파일로 구성됨 기본적으로 log.segment.bytes 설정 (기본값: 1GB)마다 새로운 세그먼트 파일 생성 Partition당 오직 하나의 Segment가 Active 되어 있음 각 세그먼트는 다음 3가지 파일로 구성됨: File name Description .log 메세지 본문이 저장된 파일 .index offset -\u0026gt; file 위치 매핑(binary) .timeindex timestamp -\u0026gt; offset 매핑(시간 기준 조회용) "},{"id":20,"href":"/docs/data-integration/kafka/kafka-storage-performance-test/","title":"Kafka의 성능 측정 (Block storage vs NFS)","section":"Kafka","content":" 개요 # Kafka docs에는 어디에 데이터를 저장해야한다는 글이 없지만, Kafka를 k8s에 올릴 때 사용하는 strimzi operator에는 다음과 같은 문구가 있다.\nEfficient data storage is essential for Strimzi to operate effectively, and block storage is strongly recommended. Strimzi has been tested only with block storage, and file storage solutions like NFS are not guaranteed to work.\n기존에는 k8s에서 kafka의 저장소로 nfs를 사용했던 터라, block storage를 추가로 깔아야한다고 생각했고 이에 따라 longhorn, openebs, ceph 중 ceph가 가장 대중적(?)이면서 de-factor standard로 알고있어서 채요했다. 물론 ceph의 여러 storage가 있지만 그 중 ceph rbd만을 사용했으며, nfs와의 성능 테스트에 대한 글이다.\n성능 테스트를 위한 시나리오는 크게 6가지로 잡았다.\n시나리오 ID 테스트 목적 입력 조건 기대 출력 (측정 지표) S1 단일 Producer Throughput 1KB × 10만건, throughput=-1 초당 처리량 (MB/s), 평균 전송 지연 S2 병렬 Producer 처리량 Producer 5개 총 처리량 (MB/s), Broker 부하 S3 단일 Consumer 처리 속도 test-topic의 10만 메시지 총 처리 시간, 초당 소비량 S4 장시간 부하 → 리소스 누수 1시간 지속 전송 (throughput=10000/s) 메모리 증가 S5 대용량 메시지 처리 100KB, 10만건, throughput 제한 없음 처리량(MB/s), 전송/소비 실패율 S6 Producer latency 측정 acks=all, linger.ms=5, batch.size=32KB 평균/최대 latency(ms) 이제 코드와 결과 및 분석 내용에 대해 순차적으로 나열하겠다.\nS1. 단일 Producer Throughput 측정 # bin/kafka-producer-perf-test.sh \\ --topic test-topic \\ --num-records 100000 \\ --record-size 1024 \\ --throughput -1 \\ --producer-props bootstrap.servers=\u0026lt;bootstrap\u0026gt; S1 - 결과 # 메시지 수 Throughput (NFS) Avg Latency (NFS) Throughput (Ceph) Avg Latency (Ceph) 100 0.06 MB/sec 270.57 ms 0.16 MB/sec 121.25 ms 500 0.25 MB/sec 678.76 ms 0.70 MB/sec 99.58 ms 1000 0.32 MB/sec 1,187.67 ms 1.00 MB/sec 258.99 ms 3000 0.39 MB/sec 3,355.14 ms 1.78 MB/sec 559.59 ms 5000 0.40 MB/sec 5,602.60 ms 2.11 MB/sec 802.07 ms 10000 0.41 MB/sec 11,196.89 ms 2.31 MB/sec 1,813.02 ms Ceph RBD는 처리량(Throughput)이 NFS 대비 4~6배 이상 높고, latency도 현저히 낮음 NFS는 쓰기 IOPS 한계와 fsync() 동기화 지연으로 인해 latency가 선형적으로 증가 Ceph은 병렬 쓰기 처리 능력이 훨씬 뛰어나 throughput을 일정 수준 이상으로 유지함 클러스터 | Kafka UI Topic Size | 메시지 수 NFS | 약 63 MB | 19,000 Ceph | 약 383 MB | 19,000\nS2. 다중 Producer Throughput 측정 # for i in {1..5}; do echo \u0026#34;[INFO] Starting producer $i\u0026#34; bin/kafka-producer-perf-test.sh \\ --topic test-topic \\ --num-records 100000 \\ --record-size 1024 \\ --throughput -1 \\ --producer-props bootstrap.servers=\u0026lt;bootstrap\u0026gt; \\ S2 - 결과 # Storage 메시지 수 Avg Throughput (MB/s) Std Avg Latency (ms) Std Ceph 100 145.41 ±4.58 75.62 ±10.33 Ceph 500 524.29 ±25.50 171.46 ±20.00 Ceph 1,000 1079.18 ±71.19 160.37 ±32.10 Ceph 3,000 1497.76 ±97.84 593.03 ±76.98 Ceph 5,000 1773.06 ±70.65 1002.05 ±64.60 Ceph 10,000 1117.00 ±6.63 3967.63 ±110.83 Ceph 50,000 1355.23 ±4.85 14715.06 ±182.03 - - - - - - NFS 100 65.54 ±11.68 40.95 ±97.09 NFS 500 106.50 ±5.61 1,940.27 ±122.21 NFS 1,000 399.04 ±14.09 976.34 ±67.71 NFS 3,000 446.17 ±6.79 3000.52 ±129.00 NFS 5,000 428.52 ±3.59 5285.97 ±107.04 NFS 10,000 439.79 ±2.63 10777.37 ±152.69 NFS 50,000 425.29 ±2.34 51486.65 ±539.49 ✅ Ceph RBD는 전 구간에서 NFS 대비 3~4배 이상 우수한 성능\nThroughput은 NFS 대비 3~4배 이상 높음 Latency는 NFS 대비 4~10배 이상 낮음 ✅ NFS는 메시지 수 증가에 따라 Latency가 선형적으로 악화\n디스크 flush 주기와 fsync() 동기화 지연이 누적되어 전체 처리 시간 증가 초당 처리량은 일정 수준(400~440 MB/s)에서 정체, 스로틀링 발생 ✅ Ceph은 병렬 쓰기 구조로 throughput 유지력 우수\n메시지 수 증가에도 일정 수준 throughput 유지 (특히 50,000건에서 안정적) 병렬 처리, OSD 분산 저장, 빠른 segment flush 구조가 원인 S3. Consumer 처리 시간 # time bin/kafka-consumer-perf-test.sh \\ --broker-list \u0026lt;bootstrap\u0026gt; \\ --topic test-topic \\ --messages 100000 \\ S3 - 결과 # 메시지 수 Ceph Throughput (msg/s) NFS Throughput (msg/s) 차이 배율 비고 100 2,994.0 123.5 24.2× 초기 메시지 수 적음에도 latency 차이 큼 500 114.2 123.7 0.92× NFS가 소폭 빠름 1000 124.5 44.1 2.8× 초기 메시지 수 적음에도 latency 차이 큼 3000 740.1 766.6 1.0× 거의 비슷 5000 1261.6 1270.3 1.0× 거의 비슷 10000 2686.9 2419.9 1.1× Ceph이 더 빠름 50000 8797.9 8732.2 1.0× 동일 수준 100000 13595.7 13653.5 0.99× NFS가 소폭 빠름 ✅ Ceph RBD는 전체적으로 안정적이지만, Consumer 테스트에선 Throughput에서 NFS와 큰 차이는 없음. 그러나 Fetch Time과 Rebalance Time에서 Ceph이 일관되고 짧은 시간을 보임.\nFetch Time : Kafka Consumer가 Broker로부터 message를 가져오는 데 걸리는 시간 Rebalance Time : Kafka Consumer Group에서 새로운 Consumer가 join하거나 leave 할 때, 파티션이 재할당되며 이 과정에서 걸리는 시간 S4. 장시간 부하테스트 # time bin/kafka-producer-perf-test.sh \\ --topic test-topic \\ --num-records 3600000 \\ --record-size 1024 \\ --throughput 1000 \\ --producer-props bootstrap.servers=\u0026lt;bootstrap\u0026gt; \\ S4 - 결과 # 항목 NFS Ceph RBD 차이 분석 전송 총량 3,600,000 records 3,600,000 records 동일 전송 속도 (records/sec) 405.74 999.98 Ceph이 약 2.46배 빠름 데이터 전송량 (MB/sec) 0.40 MB/sec 0.98 MB/sec Ceph이 2.45배 높음 평균 Latency 75,165.08 ms 89.75 ms NFS가 837배 느림 최대 Latency 83,580.00 ms 1,923.00 ms NFS가 43.5배 느림 50th Percentile (p50) 75,989 ms 26 ms NFS가 2,922배 느림 95th Percentile (p95) 80,636 ms 502 ms NFS가 160배 느림 99th Percentile (p99) 82,362 ms 824 ms NFS가 99.9배 느림 99.9th Percentile (p99.9) 82,991 ms 1,326 ms NFS가 62.5배 느림 S5. 대용량 메세지 처리 테스트 # bin/kafka-producer-perf-test.sh \\ --topic test-topic-large \\ --num-records 100000 \\ --record-size 100000 \\ --throughput -1 \\ --producer-props bootstrap.servers=\u0026lt;bootstrap\u0026gt; 총평 # 항목 Ceph RBD NFS Throughput ✔ 최대치 거의 도달 (안정적 유지) ❌ 400~440 records/sec에서 정체 Latency ✔ 10~20ms 수준, 전 구간 안정적 ❌ 수십~수천 ms, 메시지 수 증가 시 악화 병렬 처리 ✔ OSD 기반 분산 쓰기, flush 효율 높음 ❌ sync/flush 병목, 선형적 지연 발생 자원 사용 (CPU) ✔ 더 많이 쓰고 성능 확보 ❌ 적게 쓰지만 처리 성능 낮음 운영 적합성 ✔ 실시간 스트리밍 / 고부하 환경 ❌ dev/test 등 저부하 용도에 한정 Ceph RBD는 모든 구간에서 NFS 대비 3~10배 이상 우수 Kafka의 특성상, 저장소 선택이 성능과 안정성에 직접 영향 NFS는 실시간 처리 시스템에 부적합 운영 환경에서는 반드시 block 기반 고성능 스토리지 사용 권장 이 테스트를 마칠 떄 쯤 한가지 깨달은게 있었다. k8s 클러스터 외부에 두는게 가장 성능은 좋지만, \u0026ldquo;성능\u0026quot;만 생각한다면 굳이 block storage tool을 안쓰고도 Local PV를 쓸 수 있고 이를 잘만 백업해둔다면 \u0026ldquo;장애\u0026rdquo; 측면도 무리가 없다는 것을\u0026hellip;\n"},{"id":21,"href":"/docs/data-integration/kafka/kafka-storage/","title":"Kafka의 저장소","section":"Kafka","content":"Kubernetes에서 kafka를 사용할때 kafka의 데이터 저장소는 어떤 것이 적절한지에 대한 글이다.\nEfficient data storage is essential for Strimzi to operate effectively, and block storage is strongly recommended. Strimzi has been tested only with block storage, and file storage solutions like NFS are not guaranteed to work.\nStrimzi는 block storage만을 대상으로 테스트 및 인증했으며, NFS 기반 스토리지는 다음과 같은 이유로 사용을 권장하지 않음\n동시 접근 시 locking 이슈 발생 rename, append 등 Kafka가 요구하는 파일 시스템 특성과 충돌 가능 Latency 및 Throughput 불안정성 (특히 segment flush, recovery 시 심각) File handle 누수나 lease 문제가 발생할 수 있음 Kafka의 저장 방식 순서 # log message를 segment file에 append memory flush → fsync to disk segment가 일정 크기 도달 시 rename(segment.000.log → segment.001.log) index 파일 동기화 및 offset 확인 restart 시 파일 정합성 체크 → 위 작업들은 block-level consistency와 atomic operation 보장을 전제로 설계됨\n즉, Kafka는 log-segment 기반으로\n데이터는 지속적으로 append-write 되고 주기적으로 fsync, flush, segment 파일 rename 작업이 발생 재시작 시 segment recovery를 위해 .log, .index 파일의 정합성이 매우 중요 하지만… NFS는\nclient마다 lock 동작 방식 다름 → flock이나 fcntl 무력화 가능 파일 locking과 rename에 제약이 있으며 Kafka는 재시작 시 log/index 파일을 복구 → NFS는 file handle 누락, incomplete rename으로 인해 fail 가능 write latency가 높고, IOPS가 낮은 경우가 많음 -\u0026gt; Kafka의 안정성과 성능에 치명적일 수 있음 "},{"id":22,"href":"/docs/infra/kubernetes/install-with-kubespray/","title":"Kubespray로 쉽게 구축하기","section":"Kubernetes","content":" 🕒 작성일: 2025-04-15T17:35:26+09:00 ✍️ 작성자: SteveArseneLee\n모든 인스턴스에 접근할 수 있는 곳에서 다음과 같은 작업을 실행한다. In my case, it\u0026rsquo;s control node.\nKubespray Downlaod \u0026amp; venv setting # 필요한 패키지 설치 sudo apt update sudo apt install -y git python3-venv python3-pip # Kubespray 다운로드 git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray # Python 가상환경 설정 python3 -m venv k8s-venv source k8s-venv/bin/activate # 의존성 설치 pip install -r requirements.txt Inventory 구성 # 샘플 inventory 복사 cp -rfp inventory/sample inventory/mycluster # 편집기로 inventory 구성 파일 수정 vim inventory/mycluster/inventory.ini inventory.ini는 다음과 같다.\n[kube_control_plane] k8s-m1 ansible_host=192.168.0.101 ip=192.168.0.101 k8s-m2 ansible_host=192.168.0.102 ip=192.168.0.102 k8s-m3 ansible_host=192.168.0.103 ip=192.168.0.103 [etcd] k8s-m1 k8s-m2 k8s-m3 [kube_node] k8s-w1 ansible_host=192.168.0.104 ip=192.168.0.104 k8s-w2 ansible_host=192.168.0.105 ip=192.168.0.105 k8s-w3 ansible_host=192.168.0.106 ip=192.168.0.106 k8s-w4 ansible_host=192.168.0.107 ip=192.168.0.107 k8s-w5 ansible_host=192.168.0.108 ip=192.168.0.108 k8s-w6 ansible_host=192.168.0.109 ip=192.168.0.109 k8s-w7 ansible_host=192.168.0.110 ip=192.168.0.110 [k8s_cluster:children] kube_control_plane kube_node [all:vars] ansible_user=ubuntu ssh 접속 확인 및 ping test # passwordless ssh 접속 가능한지 확인 ansible -i inventory/mycluster/inventory.ini all -m ping 모든 인스턴스에 다음 명령어 실행 ubuntu 사용자에게 sudo 비밀번호 없이 실행 허용 echo \u0026#39;ubuntu ALL=(ALL) NOPASSWD:ALL\u0026#39; | sudo tee /etc/sudoers.d/ubuntu Cluster install ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b --become-user=root "},{"id":23,"href":"/docs/infra/kvm/storage-settings/","title":"KVM에서 사용할 Storage 설정","section":"KVM","content":" 1. Logical Volume 생성 # 물리 저장소에 논리 볼륨을 먼저 할당해준다\nsudo lvcreate -L {크기 e.g. 300G} -n {LV이름 e.g. kvm-pool-lv} {Volume Group 이름 e.g. ubuntu-vg} 2. 파일 시스템 생성 (ext4) # sudo mkfs.ext4 /dev/ubuntu-vg/kvm-pool-lv 3. 마운트 dir 생성 # sudo mkdir -p /mnt/kvm-pool 4. fstab 등록 (자동 마운트 설정) # echo \u0026#39;/dev/ubuntu-vg/ceph-pool-lv /mnt/ceph-pool ext4 defaults 0 2\u0026#39; | sudo tee -a /etc/fstab sudo mount -a 5. 권한 변경(libvirt가 접근 가능하게) # sudo chown libvirt-qemu:kvm /mnt/ceph-pool sudo chmod 770 /mnt/ceph-pool 6. libvirt Storage Pool 등록 (virsh) # sudo virsh pool-define-as ceph-pool dir - - - - /mnt/ceph-pool sudo virsh pool-build ceph-pool sudo virsh pool-start ceph-pool sudo virsh pool-autostart ceph-pool "},{"id":24,"href":"/docs/observability/metrics/prom-test/","title":"Prom","section":"Metrics","content":" Producer Acks, Batch, Page Cache and Flush # acks 설정은 요청이 성공할 때를 정의하는 데 사용되는 Producer에 설정하는 Parameter\nacks=0 : ack가 필요하지 않음. 이 수준은 자주 사용되지 않음. 메시지 손실이 다소 있더라도 빠르게 메시지를 보내야 하는 경우에 사용된다. acks=1(default) : Leader가 메시지를 수신하면 ack를 보냄. Leader가 Producer에게 ACK를 보낸 후, Follower가 복제하기념전에 Leader에 장애가 발생하면 메시지가 손실. “At most once(최대 한 번)” 전송을 보장 acks=-1(acks=all) : 메시지가 Leader가 모든 Replica까지 Commit 되면 ack를 보냄 Leader를 잃어도 데이터가 살아남을 수 있도록 보장. 그러나 대기 시간이 더 길고 특정 실패 사례에서 반복되는 데이터 발생 가능성 있음. “At least once(최소 한 번)” 전송을 보장 Producer Retry # retry와 관련된 Parameters:\nParameter 설명 Default 값 retries 메세지를 send하기 위해 재시도하는 횟수 MAX_INT retry.backoff.ms 재시도 사이에 추가되는 대기 시간 100 request.timeout.ms Producer가 응답을 기다리는 최대 시간 30,000(30초) delivery.timeout.ms send() 후 성공 또는 실패를 보고하는 시간의 상한 120,000(2분) =\u0026gt; retries를 조정하는 대신 delivery.timeout.ms 조정으로 retries 제어 Producer Batch 처리 # 메시지를 모아서 한번에 전송 Batch 처리는 RPC(Remote Procedure Call)수를 줄여서 Broker가 처리하는 작업이 줄어들기 때문에 더 나은 처리량을 제공 linger.ms : (default : 0). 메시지가 함께 Batch 처리될 때까지 대기 시간 batch.size : (default : 16KB). 보내기 전 Batch의 최대 크기 Batch 처리의 일반적인 설정은 linger.ms=100 및 batch.size=1000000\nPage Cache와 Flush # 메시지는 Partition에 기록됨 Partition은 Log Segment file로 구성 (기본값 : 1GB마다 새로운 Segment 생성) 성능을 위해 Log Segment는 OS Page Cache에 기록됨 로그 파일에 저장된 메시지의 데이터 형식은 Broker가 Producer로부터 수신한 것, 그리고 Consumer에게 보내는 것과 정확히 동일하므로, Zero-Copy(Zero-copy 전송은 데이터가, User Space에 복사되지 않고, CPU 개입 없이 Page Cache와 Network Buffer 사이에서 직접 전송되는 것을 의미. 이것을 통해 Broker Heap 메모리를 절약하고 또한 엄청난 처리량을 제공)가 가능 Page Cache는 다음과 같은 경우 디스크로 Flush됨 Broker가 완전히 종료 OS background “Flusher Thread” 실행 Flush 전에 Broker 장애가 발생하면\u0026hellip; # OS가 데이터를 디스크로 flush하기 전 broker의 시스템에 장애가 발생하면 해당 데이터가 손실됨 Partition이 Replication되어 있다면, Broker가 다시 온라인 상태가 되면 필요시 Leader Replica에서 데이터가 복구됨 Replication이 없다면, 데이터는 영구적 손실 가능 Kafka 자체 Flush 정책 # 마지막 Flush 이후의 메시지 수(log.flush.interval.messages) 또는 시간(log.flush.interval.ms)으로 Flush(fsync)를 트리거하도록 설정할 수 있음 Kafka는 운영 체제의 background Flush 기능(예: pdflush)을 더 효율적으로 허용하는 것을 선호하기 때문에 이러한 설정은 기본적으로 무한(기본적으로 fsync 비활성화)으로 설정 이러한 설정을 기본값으로 유지하는 것을 권장 *.log 파일을 보면 디스크로 Flush된 데이터와 아직 Flush되지 않은 Page Cache (OS Buffer)에 있는 데이터가 모두 표시됨 Flush된 항목과 Flush되지 않은 항목을 표시하는 Linux 도구(예: vmtouch)도 있음 Replica Failure # Replica Recovery # Consumer Rebalance # Partition Assignment Strategy # Cooperative Sticky Assignor # Kafka Log File # Log Retention and Cleanup Policy # Exactly Once Semantics(EOS) # "},{"id":25,"href":"/docs/others/todolist/","title":"Todo List","section":"Others","content":" Todo\u0026hellip; # Ingress 설정 Test Kafka 올리기 Done # Ceph 올리기 "},{"id":26,"href":"/docs/observability/traces/tracing/","title":"Tracing에 대해서...","section":"Traces","content":" Tracing이란? # microservice가 시스템을 경유하며 transaction 을 처리하는 과정에서 발생하는 세부적인 정보 또한, 트랜잭션이 이동한 경로, 트랜잭션을 처리하는 과정에서 발생하는 대기시간과 지연시간, 병목현상이나 에러를 일으키는 원인을 문맥(context)과 로그, 태그 등의 metadata에 출력함\nTracing과 관련된 용어 # Span # 전반적인 수행 시간 정보뿐만 아니라, 각기 하위 동작의 시작과 소요 시간 정보를 알 수 있음. Span은 동작 정보, 타임라인과 부모 스팬과의 의존성을 모두 포함하는 수행 시간을 담고 있음. Span Context # 새로운 span을 생성하기 위해선 다른 스팬을 참조해야 하는데, span context 는 이때 필요한 정보를 제공함. Span context 로 표현된 메타데이터를 쓸 수 있는 Inject 와 읽을 수 있는 Extract 메서드를 제공함. 즉, Span context 는 주입과 추출을 통해 헤더에 전달되며, 전달된 Span context 에서 추출한 스팬 정보로 새로운 자식 스팬을 생성할 수 있음. Span Reference # 두 스팬 사이의 인과관계를 가리킴 스팬 사이의 인과관계를 정의하고, 스팬들이 동일한 추적에 속한다는 것을 tracer가 알 수 있게 하는 것. 이렇나 관계는 span reference 로 나타남. Span reference 에는 child of와 following from의 두 가지 유형이 있음. e.g. Span B는 Span A의 child of이거나, Span A의 following from이라고 정의할 수 있음. "},{"id":27,"href":"/docs/infra/kvm/vm-calc/","title":"VM 사이즈 변경...","section":"KVM","content":"vm들을 k8s로 올려놨는데, 너무 부하가 큰듯하다\u0026hellip; 모든걸 다시 계산해봐야겠다.\nMain server # cpu 16, memory 64gb, ssd 1tb master 2대, worker 2대,\ntest server # cpu 22, memory 64gb, ssd 1tb master 1대, worker 3대, nfs 1대\n"},{"id":28,"href":"/docs/infra/kvm/vm-creattion-guide/","title":"VM 생성","section":"KVM","content":"[준비 단계]\nos image 다운로드 받아놓기 Resource 용량 산정 (cpu, memory, disk) ip 설계 (선택) 물리 서버가 n대라면 VM 분배 계획 [실습 단계]\nstorage pool \u0026amp; volume 생성 cockpit에서 vm 생성(이때 앞서 계획한 ip 및 자원 할당) # 1. 디렉토리 생성 및 권한 설정 sudo mkdir -p /mnt/kvm-pool sudo chown libvirt-qemu:kvm /mnt/kvm-pool sudo chmod 770 /mnt/kvm-pool # 2. 스토리지 풀 정의 및 등록 sudo virsh pool-define-as kvm-pool dir - - - - /mnt/kvm-pool sudo virsh pool-build kvm-pool sudo virsh pool-start kvm-pool sudo virsh pool-autostart kvm-pool Pool 삭제하는 법 # # 스토리지 풀 조회 sudo virsh pool-list --all # 스토리지 풀 중지 sudo virsh pool-destroy {pool명} # 스토리지 풀 정의 삭제 sudo virsh pool-undefine {pool명} Home # VM명 CPU RAM ctrl-node 2 2GB k8s-m1 2 4 k8s-m2 2 4 k8s-w1 2 8 k8s-w2 2 8 k8s-w3 2 8 VM명 Disk CPU RAM ctrl-node /mnt/kvm-pool/ctrl-node.qcow2 2 2GB k8s-m1 /mnt/kvm-pool/k8s-m1.qcow2 2 4GB k8s-m2 /mnt/kvm-pool/k8s-m2.qcow2 2 4GB k8s-w3 /mnt/kvm-pool/k8s-w3.qcow2 2 4GB Test # VM명 CPU RAM k8s-m3 2 4GB k8s-w4 2 8GB k8s-w5 2 8GB k8s-w6 2 8GB k8s-w7 2 8GB nfs-server 1 2GB VM명 Disk CPU RAM HOST k8s-m3 /mnt/kvm-pool/k8s-m3.qcow2 2 4GB k8s-w1 /mnt/kvm-pool/k8s-w1.qcow2 2 4GB k8s-w2 /mnt/kvm-pool/k8s-w2.qcow2 2 4GB nfs-server /mnt/kvm-pool/nfs.qcow2 1 2GB "},{"id":29,"href":"/docs/infra/kvm/vm-install-simple/","title":"VM 생성 간단","section":"KVM","content":"Home에는\n#!/bin/bash set -e echo \u0026#34;[HOME SERVER - STORAGE POOL \u0026amp; VOLUME SETUP]\u0026#34; # [1] Storage Pools 생성 echo \u0026#34;[1] Creating storage pools...\u0026#34; sudo mkdir -p /mnt/kvm-pool /mnt/ceph-pool sudo chown libvirt-qemu:kvm /mnt/kvm-pool /mnt/ceph-pool sudo chmod 770 /mnt/kvm-pool /mnt/ceph-pool sudo virsh pool-define-as --name kvm-pool --type dir --target /mnt/kvm-pool sudo virsh pool-build kvm-pool sudo virsh pool-start kvm-pool sudo virsh pool-autostart kvm-pool sudo virsh pool-define-as --name ceph-pool --type dir --target /mnt/ceph-pool sudo virsh pool-build ceph-pool sudo virsh pool-start ceph-pool sudo virsh pool-autostart ceph-pool # [2] Volume 생성 echo \u0026#34;[2] Creating VM OS disks...\u0026#34; cd /mnt/kvm-pool sudo qemu-img create -f qcow2 ctrl-node.qcow2 40G sudo qemu-img create -f qcow2 k8s-m1.qcow2 30G sudo qemu-img create -f qcow2 k8s-m2.qcow2 30G sudo qemu-img create -f qcow2 k8s-w1.qcow2 50G sudo qemu-img create -f qcow2 k8s-w2.qcow2 50G echo \u0026#34;[3] Creating Ceph raw disks...\u0026#34; cd /mnt/ceph-pool sudo qemu-img create -f raw k8s-w1-ceph.img 100G sudo qemu-img create -f raw k8s-w2-ceph.img 100G echo \u0026#34;[HOME SERVER SETUP COMPLETE]\u0026#34; Test에는\n#!/bin/bash set -e echo \u0026#34;[TEST SERVER - STORAGE POOL \u0026amp; VOLUME SETUP]\u0026#34; # [1] Storage Pools 생성 echo \u0026#34;[1] Creating storage pools...\u0026#34; sudo mkdir -p /mnt/kvm-pool /mnt/ceph-pool /mnt/nfs-pool sudo chown libvirt-qemu:kvm /mnt/kvm-pool /mnt/ceph-pool /mnt/nfs-pool sudo chmod 770 /mnt/kvm-pool /mnt/ceph-pool /mnt/nfs-pool # kvm-pool (OS용) sudo virsh pool-define-as --name kvm-pool --type dir --target /mnt/kvm-pool sudo virsh pool-build kvm-pool sudo virsh pool-start kvm-pool sudo virsh pool-autostart kvm-pool # ceph-pool (Ceph용) sudo virsh pool-define-as --name ceph-pool --type dir --target /mnt/ceph-pool sudo virsh pool-build ceph-pool sudo virsh pool-start ceph-pool sudo virsh pool-autostart ceph-pool # kvm-nfs-pool (NFS 서버용) sudo virsh pool-define-as --name kvm-nfs-pool --type dir --target /mnt/kvm-nfs-pool sudo virsh pool-build kvm-nfs-pool sudo virsh pool-start kvm-nfs-pool sudo virsh pool-autostart kvm-nfs-pool # [2] Volume 생성 echo \u0026#34;[2] Creating VM OS disks...\u0026#34; cd /mnt/kvm-pool sudo qemu-img create -f qcow2 k8s-m3.qcow2 30G sudo qemu-img create -f qcow2 k8s-w3.qcow2 50G sudo qemu-img create -f qcow2 k8s-w4.qcow2 50G sudo qemu-img create -f qcow2 k8s-w5.qcow2 50G sudo qemu-img create -f qcow2 nfs.qcow2 30G echo \u0026#34;[3] Creating Ceph raw disks...\u0026#34; cd /mnt/ceph-pool sudo qemu-img create -f raw k8s-w3-ceph.img 100G sudo qemu-img create -f raw k8s-w4-ceph.img 100G sudo qemu-img create -f raw k8s-w5-ceph.img 100G echo \u0026#34;[4] Creating NFS data disk...\u0026#34; cd /mnt/nfs-pool sudo qemu-img create -f qcow2 nfs-data.qcow2 300G echo \u0026#34;[TEST SERVER SETUP COMPLETE]\u0026#34; "},{"id":30,"href":"/docs/troubleshooting/resource-issue/","title":"리소스 이슈 인한 인프라 재구축","section":"트러블슈팅","content":" 상황 # 다음과 같은 이슈가 발생했다. 현재 구축된 스택은 ceph, minio, kafka, kafka-ui뿐인데, request\u0026amp;limit을 최적화했음에도 내 환경에서는 다소 버거운 것으로 보인다. 따라서 다음과 같은 결정을 하게됐다.\n문제 원인 # 위 스택들의 request를 다음과 같이 최소화했을때\n항목 Before After Ceph OSD 5개 CPU 요청 2.5코어 1.5코어 Kafka Pod 3개 CPU 요청 1.5코어 0.9코어 Kafka-UI 없음 0.1코어 총합 감소량 약 2.5~3코어 1.5코어 이상 확보 아래와 같이 리소스를 사용한다.\nubuntu@ctrl-node:~/kafka$ kubectl describe nodes | grep -A5 \u0026#34;Allocated resources\u0026#34; Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1050m (43%) 1 (41%) memory 370760Ki (5%) 1280288k (17%) -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 900m (37%) 0 (0%) memory 210800640 (2%) 1024288k (13%) -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 800m (23%) 0 (0%) memory 137400320 (1%) 709715200 (9%) -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1675m (69%) 1700m (70%) memory 2987972608 (18%) 6346859776 (40%) -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 2625m (77%) 2800m (82%) memory 4957750Ki (32%) 9165432064 (57%) -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 2675m (78%) 2300m (67%) memory 4752950Ki (30%) 10104956160 (63%) -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 2625m (77%) 2800m (82%) memory 4884022Ki (31%) 10373391616 (65%) -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 2375m (69%) 1400m (41%) memory 3835446Ki (24%) 8494343424 (53%) 현실적으로 턱없이 부족한 리소스다.\n해결 방법 # 사실 해결을 했다기보단, 현실적인 대안을 찾게됐다.\nWorker node 개수를 4대로 줄이고 리소스 추가 할당 Cloud Storage 도입 항목 AWS GCP Azure Block Storage EBS (가장 안정적, 옵션 다양) Persistent Disk (속도 좋음, 가격 저렴) Managed Disk (무난함) Object Storage S3 (업계 표준, 생태계 최강) GCS (가격 저렴, 성능 빠름) Blob Storage (무난) File Storage EFS (비쌈) Filestore (가격 적당, 성능 빠름) Azure Files (가장 저렴) 총평 \u0026ldquo;기업표준\u0026rdquo; \u0026amp; 비쌈 \u0026ldquo;가격/성능 균형\u0026rdquo; 최고 \u0026ldquo;비용 최저\u0026rdquo; 목적이면 고려 비용만 생각한다면 Azure가 최선이지만, 편의성을 고려하면 AWS나 GCP를 선택해야한다. 따라서 GCP의 스토리지들을 채택했다.\nStorage Type Product Block Storage Balanced Persistent Disk (pd-balanced) Object Storage GCS Standard File Storage Filestore Basic HDD "},{"id":31,"href":"/docs/infra/kubernetes/context-switch/","title":"여러 환경의 클러스터를 손쉽게 관리하기","section":"Kubernetes","content":" 1. 원격 클러스터의 kubeconfig 받아오기 # sudo cat /etc/kubernetes/admin.conf 2. 로컬 파일에 저장 내 경우 # ~/kubeconfig |- kubeconfig-cluster1 |- kubeconfig-cluster2 |- ... 이런식으로 저장했음.\n3. context, cluster, name이 중복되면 안돼서 변환해주는 스크립트 사용 # auto-context.sh\n#!/bin/bash set -e if [ $# -ne 1 ]; the원 echo \u0026#34;❗ 사용법: $0 \u0026lt;kubeconfig-파일 경로\u0026gt;\u0026#34; exit 1 fi INPUT_PATH=\u0026#34;$1\u0026#34; FILENAME=$(basename \u0026#34;$INPUT_PATH\u0026#34;) CLUSTER_ID=\u0026#34;${FILENAME#kubeconfig-}\u0026#34; echo \u0026#34;📌 클러스터 ID: $CLUSTER_ID\u0026#34; echo \u0026#34;📂 대상 kubeconfig: $INPUT_PATH\u0026#34; # 🔄 기존 등록 삭제 kubectl config delete-context ${CLUSTER_ID}-context 2\u0026gt;/dev/null || true kubectl config delete-cluster ${CLUSTER_ID}-cluster 2\u0026gt;/dev/null || true kubectl config delete-user ${CLUSTER_ID}-user 2\u0026gt;/dev/null || true # 🔍 정보 추출 SERVER=$(KUBECONFIG=$INPUT_PATH kubectl config view -o jsonpath=\u0026#34;{.clusters[0].cluster.server}\u0026#34;) CERT_AUTH_DATA=$(KUBECONFIG=$INPUT_PATH kubectl config view -o jsonpath=\u0026#34;{.clusters[0].cluster.certificate-authority-data}\u0026#34;) CLIENT_CERT_DATA=$(KUBECONFIG=$INPUT_PATH kubectl config view -o jsonpath=\u0026#34;{.users[0].user.client-certificate-data}\u0026#34;) CLIENT_KEY_DATA=$(KUBECONFIG=$INPUT_PATH kubectl config view -o jsonpath=\u0026#34;{.users[0].user.client-key-data}\u0026#34;) # 🔧 base64 decode → temp 파일 CA_FILE=$(mktemp) CERT_FILE=$(mktemp) KEY_FILE=$(mktemp) echo \u0026#34;$CERT_AUTH_DATA\u0026#34; | base64 -d \u0026gt; \u0026#34;$CA_FILE\u0026#34; echo \u0026#34;$CLIENT_CERT_DATA\u0026#34; | base64 -d \u0026gt; \u0026#34;$CERT_FILE\u0026#34; echo \u0026#34;$CLIENT_KEY_DATA\u0026#34; | base64 -d \u0026gt; \u0026#34;$KEY_FILE\u0026#34; # ✅ 완전 등록 (PEM 파일 방식) kubectl config set-cluster ${CLUSTER_ID}-cluster \\ --server=\u0026#34;$SERVER\u0026#34; \\ --certificate-authority=\u0026#34;$CA_FILE\u0026#34; \\ --embed-certs=true kubectl config set-credentials ${CLUSTER_ID}-user \\ --client-certificate=\u0026#34;$CERT_FILE\u0026#34; \\ --client-key=\u0026#34;$KEY_FILE\u0026#34; \\ --embed-certs=true kubectl config set-context ${CLUSTER_ID}-context \\ --cluster=${CLUSTER_ID}-cluster \\ --user=${CLUSTER_ID}-user # 🔁 병합 echo \u0026#34;🔁 병합 중...\u0026#34; MERGE_TARGETS=$(find ~/kubeconfig -type f -name \u0026#39;kubeconfig-*\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39;:\u0026#39; | sed \u0026#39;s/:$//\u0026#39;) KUBECONFIG=$MERGE_TARGETS kubectl config view --flatten \u0026gt; ~/.kube/config chmod 600 ~/.kube/config # 🧹 임시 파일 정리 rm -f \u0026#34;$CA_FILE\u0026#34; \u0026#34;$CERT_FILE\u0026#34; \u0026#34;$KEY_FILE\u0026#34; echo \u0026#34;✅ 병합 완료: ~/.kube/config 갱신됨\u0026#34; 4. config 파일 수정 # config 파일을 처음 받아오면 certificate-authority-data 등의 데이터에 DATA+OMITTED라고 써있는 경우가 있다. 이런 경우 실제 데이터로 바꿔준다.\nbase64 -w 0 /etc/kubernetes/pki/ca.crt # 서버에서 추출해서 5. 이제 위 config를 사용해 자동으로 context를 바꾸기 # /usr/local/bin/usr에 sc라는 파일을 만들어주고\n#!/bin/zsh autoload -Uz colors \u0026amp;\u0026amp; colors typeset -A CLUSTERS CLUSTERS=( 1 context1 2 context2 3 context3 ) typeset -A DESCRIPTIONS DESCRIPTIONS=( 1 \u0026#34;🎯 Description of context1\u0026#34; 2 \u0026#34;🧪 Description of context2\u0026#34; 3 \u0026#34;💻 Description of context3\u0026#34; ) ORDER=(1 2 3) # 순서 고정 current_context=$(kubectl config current-context 2\u0026gt;/dev/null) echo \u0026#34;\u0026#34; echo \u0026#34;${fg[cyan]}🌐 Kubernetes Context Switcher v3.0${reset_color}\u0026#34; echo \u0026#34;${fg[white]}──────────────────────────────────────────────${reset_color}\u0026#34; for key in \u0026#34;${ORDER[@]}\u0026#34;; do ctx=${CLUSTERS[$key]} desc=${DESCRIPTIONS[$key]} if [[ \u0026#34;$ctx\u0026#34; == \u0026#34;$current_context\u0026#34; ]]; then printf \u0026#34; %s) ${fg[green]}%s${reset_color} %s ${fg[yellow]}(현재 선택됨)${reset_color}\\n\u0026#34; \u0026#34;$key\u0026#34; \u0026#34;$ctx\u0026#34; \u0026#34;$desc\u0026#34; else printf \u0026#34; %s) ${fg[white]}%s${reset_color} %s\\n\u0026#34; \u0026#34;$key\u0026#34; \u0026#34;$ctx\u0026#34; \u0026#34;$desc\u0026#34; fi done echo \u0026#34;${fg[white]}──────────────────────────────────────────────${reset_color}\u0026#34; echo \u0026#34;\u0026#34; read \u0026#34;choice?🔢 번호 입력: \u0026#34; context=\u0026#34;${CLUSTERS[$choice]}\u0026#34; if [[ -n \u0026#34;$context\u0026#34; ]]; then echo \u0026#34;\u0026#34; echo \u0026#34;${fg[blue]}🔁 전환 중...${reset_color}\u0026#34; kubectl config use-context \u0026#34;$context\u0026#34; echo \u0026#34;${fg[blue]}📡 클러스터 상태 확인 중...${reset_color}\u0026#34; if kubectl get nodes \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;${fg[green]}✅ 연결 성공!${reset_color}\u0026#34; else echo \u0026#34;${fg[red]}❌ 연결 실패. VPN이나 네트워크 상태를 확인하세요.${reset_color}\u0026#34; fi else echo \u0026#34;${fg[red]}❌ 유효하지 않은 선택입니다.${reset_color}\u0026#34; exit 1 fi "},{"id":32,"href":"/docs/%EC%BB%B4%ED%93%A8%ED%84%B0%EC%9D%B4%EB%A1%A0/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4/normalization/","title":"정규화 - 1NF, 2NF, 3NF, BCNF","section":"데이터베이스","content":" 정규화란? # 데이터를 효율적으로 관리하기 위해 데이터를 여러 테이블로 나누고, 데이터 중복과 이상 현상을 최소화해 데이터의 일관성을 유지하는 과정\n정규화의 목표를 정리하자면 다음과 같다.\n데이터 중복 줄이기 : 불필요하게 반복해서 저장하지 않게 함 데이터 무결성 유지 : 삽입, 삭제, 갱신 시 일관성 있게 데이터가 유지됨 이상 현상 방지 : 삽입, 삭제, 갱신 시 발생 가능한 Anomalies 방지 DB 성능 최적화 : 필요한 데이터만 저장하고, 데이터를 효율적으로 검색하고 수정 가능함 이상 현상이란..? # 정규화를 통해 방지하려는 주요 이상 현상들이다.\n삽입 이상(Insertion Anomaly) : 데이터 삽입 시 불필요한 데이터를 함께 삽입해야 하는 문제 갱신 이상(Update Anomaly) : 데이터의 일부만 갱신해 데이터 불일치가 발생 가능한 문제 삭제 이상(Deletion Anomaly) : 데이터를 삭제할 때 의도치 않은 정보도 함께 삭제되는 문제 정규형 # 1NF - \u0026lt;반복되는 속성 및 그룹 속성 제거\u0026gt; # 목적 : 각 컬럼이 _Atomic Value_만을 가지도록 하여, 컬럼에 여러 값이 들어가는 것을 방지한다. 조건 모든 컬럼 값이 원자값이어야 하며, 각 셀에 단일 값만 포함되어야 한다. 반복 그룹(한 컬럼에 여러 개의 값이 있는 경우)이나 중첩된 집합이 없어야 한다. 학생ID 이름 전화번호 1 김철수 010-111-222, 010-333-444 2 이영희 010-555-666 전화번호 컬럼에 여러 값이 포함되어 있으므로 다음과 같이 변경한다.\n학생ID 이름 전화번호 1 김철수 010-111-222 1 김철수 010-333-444 2 이영희 010-555-666 전화번호 컬럼을 여러 행으로 나눠 각 행이 하나의 전화번호만 포함하도록 해 원자값을 갖도록 한다.\n2NF - \u0026lt;부분 함수 종속성 제거\u0026gt; # 목적 : 부분 함수 종속성을 제거해 데이터의 중복을 줄이고, 데이터의 일관성을 유지한다. 기본 키의 일부가 아닌 전체에 종속되도록 해 데이터 변경 시 일관성을 높인다. 조건 1NF를 만족한다. 기본 키가 복합 키인 경우, 부분 함수 종속성이 없어야 한다. 즉, 비기본 키 속성이 기본 키 전체에 종속되어야 한다. 1NF만 만족한 Table\n학생ID 과목ID 과목명 교수명 1 A101 수학 김교수 1 B202 영어 박교수 2 A101 수학 김교수 2 B202 영어 박교수 과목명과 교수명은 과목ID에 종속되며, 학생ID와는 무관하다.\n즉, 부분 함수 종속이 발생해 2NF를 만족하지 않으므로 다음과 같이 변경한다.\n학생-과목 관계 Table\n학생ID 과목ID 1 A101 1 B202 2 A101 2 B202 과목 Table\n과목ID 과목명 교수명 A101 수학 김교수 B202 영어 박교수 과목ID가 과목명과 교수명을 결정하도록 테이블을 분리해 부분 함수 종속성을 제거했다. 과목명과 교수명 정보가 중복되지 않음으로써 변경 시 한 곳에서만 수정할 수 있다.\n3NF - \u0026lt;이행 함수 종속성 제거\u0026gt; # 목적 : 이행적 종속성을 제거해 데이터의 일관성을 높이고, 데이터 중복을 줄인다. 비기본 키 속성이 다른 비기본 키 속성에 의존하지 않도록 한다. 조건 2NF를 만족한다. 비기본 키 속성이 다른 비기본 키 속성에 이행적 종속성을 가지면 안된다. 과목ID 교수명 교수연락처 A101 김교수 010-111-222 B202 박교수 010-333-444 교수연락처는 교수명에 종속되어 있으며, 과목ID를 통해 간접적으로 결정된다.\n즉, 이행적 종속이 발생했으므로 다음과 같이 변경한다.\n과목 Table\n과목ID 교수명 A101 김교수 B202 박교수 교수 Table\n교수명 교수연락처 김교수 010-111-222 박교수 010-333-444 BCNF - \u0026lt;결정자가 후보키가 아닌 함수 종속성(FD) 제거\u0026gt; # 목적 : 후보 키가 아닌 속성의 의해 결정되는 문제를 제거해 데이터 무결성을 보장한다. 조건 3NF를 만족한다. 모든 결정자가 후보 키가 되도록 만든다. 비후보 키가 결정자 역할을 하는 상황을 방지한다. 복합키를 사용하는 테이블에서 발생하는 문제를 해결하는 데 활용된다 강의ID 교수ID 학생ID L101 P01 S01 L102 P02 S01 L101 P01 S02 L103 P03 S03 강의ID가 교수ID를 결정하지만, 강의ID와 학생ID의 조합이 기본 키로 사용되고 있다.\n따라서 비후보 키인 강의ID가 교수ID를 결정하는 종속성이 발생해 다음과 같이 변경한다.\n강의-교수 Table [각 강의에 대한 고정된 교수 정보를 저장]\n강의ID 교수ID L101 P01 L102 P02 L103 P03 강의-학생 Table [각 학생이 수강하는 강의 정보를 저장]\n강의ID 학생ID L101 S01 L101 S02 L102 S01 L103 S03 "},{"id":33,"href":"/docs/troubleshooting/kafka-one-operator-multi-kafka/","title":"하나의 operator와 여러 namespace의 여러 kafka cluster","section":"트러블슈팅","content":" 상황 # Strimzi Kafka Operator를 하나만 설치한 뒤, 여러 namespace에 Kafka 클러스터를 각각 배포하려는 구조로 운영하고자 했음.\nOperator는 kafka 네임스페이스에 설치됨 새 Kafka 클러스터는 kafka2 네임스페이스에 배포됨 KafkaNodePool + Kafka CR (KRaft 모드) 사용 Kafka UI도 네임스페이스별로 함께 배포 문제 원인 # KafkaNodePool 기반 배포를 시도했을 때 다음과 같은 오류가 발생:\nKafkaNodePool이 Kafka 리소스와 연결되지 않음 strimzipodsets 리소스 접근 실패 (list/create 권한 없음) Kafka 및 KafkaNodePool의 status 필드 업데이트 실패 (403 Forbidden) 이유는 다음과 같음:\nClusterRole strimzi-cluster-operator-namespaced가 기본적으로 kafka2의 리소스에 대한 권한을 가지지 않음 kafka2 네임스페이스에는 RoleBinding만 생성되어 있고, ClusterRole에는 필요한 리소스 권한이 빠져 있음 Kafka와 KafkaNodePool의 이름 불일치로 연결 실패 해결 방법 # 다음과 같은 조치를 통해 문제를 해결함:\nKafkaNodePool.labels.strimzi.io/cluster 값과 Kafka.metadata.name 값을 일치시킴 ClusterRole(strimzi-cluster-operator-namespaced)에 다음 권한을 추가: - apiGroups: [\u0026#34;kafka.strimzi.io\u0026#34;] resources: [\u0026#34;kafkas/status\u0026#34;] verbs: [\u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;kafka.strimzi.io\u0026#34;] resources: [\u0026#34;kafkanodepools/status\u0026#34;] verbs: [\u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;core.strimzi.io\u0026#34;] resources: [\u0026#34;strimzipodsets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] kubectl patch clusterrole strimzi-cluster-operator-namespaced --type=\u0026#39;json\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/rules/-\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;apiGroups\u0026#34;: [\u0026#34;kafka.strimzi.io\u0026#34;], \u0026#34;resources\u0026#34;: [\u0026#34;kafkas/status\u0026#34;], \u0026#34;verbs\u0026#34;: [\u0026#34;update\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/rules/-\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;apiGroups\u0026#34;: [\u0026#34;kafka.strimzi.io\u0026#34;], \u0026#34;resources\u0026#34;: [\u0026#34;kafkanodepools/status\u0026#34;], \u0026#34;verbs\u0026#34;: [\u0026#34;update\u0026#34;] } }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/rules/-\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;apiGroups\u0026#34;: [\u0026#34;core.strimzi.io\u0026#34;], \u0026#34;resources\u0026#34;: [\u0026#34;strimzipodsets\u0026#34;], \u0026#34;verbs\u0026#34;: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] } } ] kafka 네임스페이스의 strimzi-cluster-operator 서비스 계정이 kafka2 네임스페이스 리소스에 접근할 수 있도록 RoleBinding 구성: apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: strimzi-cluster-operator-binding namespace: kafka2 subjects: - kind: ServiceAccount name: strimzi-cluster-operator namespace: kafka roleRef: kind: ClusterRole name: strimzi-cluster-operator-namespaced apiGroup: rbac.authorization.k8s.io STRIMZI_NAMESPACE 환경변수에 kafka2 추가 kubectl set env deployment/strimzi-cluster-operator -n kafka STRIMZI_NAMESPACE=\u0026#34;kafka,kafka2\u0026#34; 요약된 스크립트 # install-kafka-on-new-ns.sh #!/bin/bash set -e # 기존 Operator 네임스페이스 OPERATOR_NS=\u0026#34;kafka\u0026#34; # 새로 배포할 네임스페이스 TARGET_NS=\u0026#34;$1\u0026#34; # Kafka 클러스터 이름 KAFKA_CLUSTER=\u0026#34;$2\u0026#34; # Kafka UI 정보 KAFKA_UI_DEPLOYMENT=\u0026#34;kafka-ui-${KAFKA_CLUSTER}\u0026#34; KAFKA_UI_SERVICE=\u0026#34;kafka-ui-${KAFKA_CLUSTER}\u0026#34; KAFKA_UI_PORT=30640 KAFKA_TIMEOUT=300s UI_TIMEOUT=60s if [[ -z \u0026#34;$TARGET_NS\u0026#34; || -z \u0026#34;$KAFKA_CLUSTER\u0026#34; ]]; then echo \u0026#34;[ERROR] Usage: $0 \u0026lt;target-namespace\u0026gt; \u0026lt;kafka-cluster-name\u0026gt;\u0026#34; exit 1 fi echo \u0026#34;🚀 Deploying Kafka cluster \u0026#39;${KAFKA_CLUSTER}\u0026#39; in namespace \u0026#39;${TARGET_NS}\u0026#39;...\u0026#34; ### [1] 네임스페이스 생성 ### kubectl get namespace ${TARGET_NS} \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || kubectl create namespace ${TARGET_NS} ### [2] 기존 STRIMZI_NAMESPACE 환경변수에 target NS 추가 ### CURRENT_NAMESPACES=$(kubectl get deployment strimzi-cluster-operator -n ${OPERATOR_NS} -o jsonpath=\u0026#39;{.spec.template.spec.containers[0].env[?(@.name==\u0026#34;STRIMZI_NAMESPACE\u0026#34;)].value}\u0026#39;) if [[ \u0026#34;$CURRENT_NAMESPACES\u0026#34; != *\u0026#34;$TARGET_NS\u0026#34;* ]]; then echo \u0026#34;🔧 Adding ${TARGET_NS} to STRIMZI_NAMESPACE\u0026#34; kubectl set env deployment/strimzi-cluster-operator -n ${OPERATOR_NS} STRIMZI_NAMESPACE=\u0026#34;${CURRENT_NAMESPACES},${TARGET_NS}\u0026#34; else echo \u0026#34;✅ STRIMZI_NAMESPACE already includes ${TARGET_NS}\u0026#34; fi ### [3] 기존 ClusterRoleBinding 규칙 확인 및 복사 적용 ### echo \u0026#34;🔑 Creating RoleBindings from existing ones in ${OPERATOR_NS}\u0026#34; for role in strimzi-cluster-operator-namespaced strimzi-entity-operator strimzi-kafka-broker; do kubectl create rolebinding strimzi-${role}-binding \\ --namespace ${TARGET_NS} \\ --clusterrole ${role} \\ --serviceaccount ${OPERATOR_NS}:strimzi-cluster-operator \\ --dry-run=client -o yaml | kubectl apply -f - done ### [4] Kafka 클러스터 배포 ### cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: dual-role namespace: ${TARGET_NS} labels: strimzi.io/cluster: ${KAFKA_CLUSTER} spec: replicas: 3 roles: - controller - broker storage: type: jbod volumes: - id: 0 type: persistent-claim size: 100Gi deleteClaim: false class: nfs-client --- apiVersion: kafka.strimzi.io/v1beta2 kind: Kafka metadata: name: ${KAFKA_CLUSTER} namespace: ${TARGET_NS} annotations: strimzi.io/node-pools: enabled strimzi.io/kraft: enabled spec: kafka: version: 3.7.0 metadataVersion: 3.7-IV4 replicas: 3 listeners: - name: plain port: 9092 type: internal tls: false - name: tls port: 9093 type: internal tls: true - name: external port: 9094 type: nodeport tls: false configuration: bootstrap: nodePort: 31000 brokers: - broker: 0 nodePort: 31001 - broker: 1 nodePort: 31002 - broker: 2 nodePort: 31003 config: offsets.topic.replication.factor: 3 transaction.state.log.replication.factor: 3 transaction.state.log.min.isr: 2 default.replication.factor: 3 min.insync.replicas: 2 entityOperator: topicOperator: {} userOperator: {} kafkaExporter: groupRegex: \u0026#34;.*\u0026#34; topicRegex: \u0026#34;.*\u0026#34; EOF ### [5] Kafka 준비 대기 ### echo \u0026#34;⏳ Waiting for Kafka cluster to be ready...\u0026#34; kubectl wait --for=condition=ready kafka/${KAFKA_CLUSTER} -n ${TARGET_NS} --timeout=${KAFKA_TIMEOUT} || echo \u0026#34;⚠️ Kafka cluster wait timed out.\u0026#34; ### [6] Kafka UI 배포 ### cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: ${KAFKA_UI_DEPLOYMENT} namespace: ${TARGET_NS} spec: replicas: 1 selector: matchLabels: app: ${KAFKA_UI_DEPLOYMENT} template: metadata: labels: app: ${KAFKA_UI_DEPLOYMENT} spec: containers: - name: kafka-ui image: provectuslabs/kafka-ui:latest ports: - containerPort: 8080 env: - name: KAFKA_CLUSTERS_0_NAME value: \u0026#34;${KAFKA_CLUSTER}\u0026#34; - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS value: \u0026#34;${KAFKA_CLUSTER}-kafka-bootstrap.${TARGET_NS}:9092\u0026#34; --- apiVersion: v1 kind: Service metadata: name: ${KAFKA_UI_SERVICE} namespace: ${TARGET_NS} spec: type: NodePort ports: - port: 80 targetPort: 8080 nodePort: ${KAFKA_UI_PORT} selector: app: ${KAFKA_UI_DEPLOYMENT} EOF ### [7] Kafka UI 준비 대기 및 접속 정보 출력 ### kubectl wait --for=condition=ready pod -l app=${KAFKA_UI_DEPLOYMENT} -n ${TARGET_NS} --timeout=${UI_TIMEOUT} || echo \u0026#34;⚠️ Kafka UI wait timed out.\u0026#34; NODE_IP=$(kubectl get nodes -o jsonpath=\u0026#39;{.items[0].status.addresses[?(@.type==\u0026#34;InternalIP\u0026#34;)].address}\u0026#39;) echo \u0026#34;============================================================\u0026#34; echo \u0026#34;✅ Kafka cluster \u0026#39;${KAFKA_CLUSTER}\u0026#39; deployed in \u0026#39;${TARGET_NS}\u0026#39;\u0026#34; echo \u0026#34;🌐 Kafka UI URL: http://$NODE_IP:$KAFKA_UI_PORT\u0026#34; echo \u0026#34;============================================================\u0026#34; 위 스크립트는 ./install-kafka-on-new-ns.sh kafka-new-ns new-kafka-cluster-name처럼 사용하면 된다.\n"}]
+++
title = "Ceph ê°œìš” ë° êµ¬ì¶•í•˜ê¸°"
draft = false
bookHidden = true
+++
## Cephì´ë€?
> ì™„ì „íˆ ë¶„ì‚°ë˜ê³ , í™•ì¥ì„±ê³¼ ê³ ê°€ìš©ì„±ì„ ì§€í–¥í•˜ëŠ” storage platform

í•µì‹¬ ëª©í‘œ
- ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°ì—ì„œ Object Storage, Block Storage, File System ëª¨ë‘ ì œê³µ
- ë¶„ì‚°ì„±, Self-Healing, ë¬´ì¤‘ë‹¨ í™•ì¥ì„ ê¸°ë³¸ ì„¤ê³„ ì² í•™ìœ¼ë¡œ ë‘ 
- í•˜ë“œì›¨ì–´ì— ì¢…ì†ë˜ì§€ ì•Šê³  x86 ì„œë²„ + disk ì¡°í•©ìœ¼ë¡œ ëŒ€ê·œëª¨ ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ëŠ¥

ì£¼ìš” íŠ¹ì§•
- CRUSH ì•Œê³ ë¦¬ì¦˜(Controlled Replication Under Scalable Hashing) ê¸°ë°˜ ë¶„ì‚° ë°ì´í„° ë§µí•‘.
- ë³„ë„ì˜ ì¤‘ì•™ ë©”íƒ€ë°ì´í„° ì„œë²„ ì—†ì´ ë°ì´í„° ìœ„ì¹˜ ê²°ì • (Object Storage).
- ì¥ì•  ë°œìƒ ì‹œ ìë™ ë³µêµ¬(Self Healing).
- ìˆ˜í‰ í™•ì¥(linear scalability) ê°€ëŠ¥ (ë…¸ë“œ ì¶”ê°€ë§Œìœ¼ë¡œ í™•ì¥).
- ë‹¤ì–‘í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ: S3 í˜¸í™˜(Object), RBD(Block), CephFS(File System).

---

### Cephì˜ êµ¬ì„±ìš”ì†Œ
|ì»´í¬ë„ŒíŠ¸|ì„¤ëª…|
|-|-|
|MON (Monitor)|Ceph í´ëŸ¬ìŠ¤í„°ì˜ ë§µ ê´€ë¦¬ì™€ Quorum(í•©ì˜)ì„ ë‹´ë‹¹.<br>- ëª¨ë“  í´ëŸ¬ìŠ¤í„° ìƒíƒœ(MAP) ì €ì¥<br>- 3ê°œ ì´ìƒ ì¶”ì²œ (í•­ìƒ í™€ìˆ˜)|
|MGR (Manager)|Cephì˜ ìƒíƒœ ëª¨ë‹ˆí„°ë§, ëª¨ë“ˆ ê´€ë¦¬ ë‹´ë‹¹.<br>- ëŒ€ì‹œë³´ë“œ, ëª¨ë‹ˆí„°ë§ í†µí•©, Prometheus Exporter ì œê³µ<br>- 1 active + 1 standby í˜•íƒœë¡œ êµ¬ì„± ì¶”ì²œ|
|OSD (Object Storage Daemon)|ì‹¤ì œ ë°ì´í„° ì €ì¥.<br>- ë¬¼ë¦¬ ë””ìŠ¤í¬ í•˜ë‚˜ë‹¹ 1 OSD í”„ë¡œì„¸ìŠ¤<br>- Ceph Pool ë³µì œ(replication), ë°ì´í„° ë³µêµ¬, ë¦¬ë°¸ëŸ°ì‹± ë‹´ë‹¹|
|MDS (Metadata Server)|CephFS(íŒŒì¼ì‹œìŠ¤í…œ) ë©”íƒ€ë°ì´í„° ê´€ë¦¬.<br>- ë¸”ë¡/ì˜¤ë¸Œì íŠ¸ëŠ” í•„ìš” ì—†ìŒ. CephFS ì“¸ ë•Œë§Œ í•„ìš”|
|RGW (Rados Gateway)|S3, Swift APIë¥¼ ì œê³µí•˜ëŠ” ê²Œì´íŠ¸ì›¨ì´.<br>- ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€ ì ‘ì†ìš© ì„œë¹„ìŠ¤|
|CSI Driver|Kubernetes PVCë¥¼ Ceph RBD, CephFSì™€ ì—°ê²°í•˜ëŠ” ì—­í• .<br>- Rook Operatorê°€ ìë™ìœ¼ë¡œ ì„¤ì¹˜|

|ì €ì¥ ë°©ì‹ | í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸|
|-|-|
|Block Storage (RBD) | MON, MGR, OSD|
|Object Storage (S3) | MON, MGR, OSD, RGW|
|File Storage (CephFS) | MON, MGR, OSD, MDS|


```sh
helm repo add {rook_repoëª…} https://charts.rook.io/release

helm install {rook-ceph_chartëª…} rook/rook-ceph --version 1.17.0
```

---
## Ceph êµ¬ì¶•í•˜ê¸°
<details markdown="1">
  <summary>ceph-cluster.yaml</summary>

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.2
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    ssl: false
  monitoring:
    enabled: false
  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false
      requireMsgr2: false
  crashCollector:
    disable: false
  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M
  cleanupPolicy:
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false
  removeOSDsIfOutAndSafeToRemove: false
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  storage:
    useAllNodes: true
    useAllDevices: true
    config:
      # í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì— ì¶”ê°€ ì„¤ì • ê°€ëŠ¥ (ex. databaseSizeMB)
    allowDeviceClassUpdate: false
    allowOsdCrushWeightUpdate: false
    scheduleAlways: false
    onlyApplyOSDPlacement: false
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
  csi:
    readAffinity:
      enabled: false
    cephfs:
      # fuseMountOptions, kernelMountOptions ì„¤ì • ê°€ëŠ¥ (ê¸°ë³¸ì€ ë¹„ì›Œë‘ )
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
    startupProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false

```
</details>

ì¢€ ë” ê°„ë‹¨í•˜ê²Œ í•˜ê³ ì‹¶ë‹¤ë©´,
<details markdown="1">
  <summary>ceph-cluster-short.yaml</summary>

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.2
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    ssl: false
  storage:
    useAllNodes: true
    useAllDevices: true
    config: {}
  disruptionManagement:
    managePodBudgets: true

```
</details>

í•˜ì§€ë§Œ í˜„ì¬ ìƒí™©ì—ì„œ ë¦¬ì†ŒìŠ¤ê°€ ì—¬ìœ ë¡­ì§„ ì•Šìœ¼ë‹ˆ,
<details markdown="1">
  <summary>ceph-cluster-limit.yaml</summary>

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.2
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false

  mgr:
    count: 1
    allowMultiplePerNode: false

  dashboard:
    enabled: true
    ssl: false

  crashCollector:  # ğŸ”» ë¹„í™œì„±í™”
    disable: true

  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M

  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "vdb"
    config:
      osdsPerDevice: "1"

  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30

  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s

  resources:  # ğŸ”§ ê° ë°ëª¬ë³„ ë¦¬ì†ŒìŠ¤ ì œí•œ
    mgr:
      limits:
        cpu: "250m"
        memory: "256Mi"
      requests:
        cpu: "100m"
        memory: "128Mi"
    mon:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "250m"
        memory: "256Mi"
    osd:
      limits:
        cpu: "1000m"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"

```
</details>


### ì ìš© í¬ì¸íŠ¸

|í•­ëª©|ì„¤ì • ë‚´ìš©|
|-|-|
|Namespace|rook-ceph|
|Ceph Version|v19.2.2 (Squid ìµœì‹  ì•ˆì •íŒ)|
|Mon Count|3ê°œ|
|Mgr Count|2ê°œ (Active + Standby)|
|OSD êµ¬ì„±|useAllDevices: true (vdb ìë™ ì¸ì‹)|
|Dashboard|í™œì„±í™”, SSL ë„ê¸° (HTTP)|
|Disk Encryption|ì‚¬ìš© ì•ˆ í•¨ (ë‚´ë¶€ë§ ê³ ë ¤)
|Monitoring|ë¹„í™œì„±í™” (Prometheus ì•„ì§ ì„¤ì¹˜ ì•ˆ ë˜ì–´ ìˆìŒ)|
|CrashCollector/LogCollector|í™œì„±í™”|
|Disruption Management|í™œì„±í™” (PodDisruptionBudget ì ìš©)|
|CleanUp Policy|ê¸°ë³¸ê°’ (ë°ì´í„° ë³´ì¡´)|
|Future Expansion|Worker, OSD ì¶”ê°€ ìš©ì´í•˜ê²Œ ì„¤ì •|

> "If you want all nodes and all available devices to be used automatically, set useAllNodes: true and useAllDevices: true. Specifying nodes: list is only necessary when you want **fine-grained control** over device selection.""
- í˜„ì¬ worker 3,4,5ë§Œ vdbê°€ ì¶”ê°€ë˜ì–´ìˆì–´ì„œ ìë™ìœ¼ë¡œ w3,w4,w5ì—ë§Œ OSDê°€ ìƒì„±ë¨
- ì¶”ê°€ë¡œ w6, w7ì— vdb ì¶”ê°€í•˜ë©´ ìë™ìœ¼ë¡œ OSDë„ í™•ì¥ë¨

|í•­ëª© | ì¼ë°˜ ì„¤ì • ì´ìœ |
|-|-|
|skipUpgradeChecks: false | ì—…ê·¸ë ˆì´ë“œ ì‹œ cluster health ì²´í¬í•¨ (ì•ˆì •ì„±)|
|allowMultiplePerNode: false | ê°™ì€ ë…¸ë“œì— MON 2ê°œ ìƒì„± ë°©ì§€ (HA ë³´ì¥)|
|dashboard.ssl: false | ë‚´ë¶€ë§ì´ë©´ SSL ë¶ˆí•„ìš”, ìš´ì˜ í¸ì˜ì„±|
|useAllDevices: true | ìë™ ë””ë°”ì´ìŠ¤ ì¸ì‹, ë””ìŠ¤í¬ ì¶”ê°€ì‹œ í™•ì¥ì„±|
|managePodBudgets: true | PodDisruptionBudget ì ìš©, ì¥ì• /ë…¸ë“œDrain ì‹œ ì•ˆì •ì„±|
|logCollector: enabled: true | ì¥ì•  ë¶„ì„ ìœ„í•´ daemon ë¡œê·¸ íŒŒì¼ ì €ì¥|
|crashCollector: disable: false | crash ë°œìƒ ì‹œ crash report ìˆ˜ì§‘|
|healthCheck: í™œì„±í™” | MON, OSD, MGR ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœ ì ê²€|
